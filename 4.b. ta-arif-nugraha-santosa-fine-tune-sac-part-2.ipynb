{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4eddd6a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:29.915583Z",
     "iopub.status.busy": "2025-06-05T13:42:29.915280Z",
     "iopub.status.idle": "2025-06-05T13:42:40.157767Z",
     "shell.execute_reply": "2025-06-05T13:42:40.156956Z"
    },
    "papermill": {
     "duration": 10.249896,
     "end_time": "2025-06-05T13:42:40.159351",
     "exception": false,
     "start_time": "2025-06-05T13:42:29.909455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Encoder ---\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "        self.encoder_dim = 2048\n",
    "\n",
    "        resnet = models.resnet101(weights='DEFAULT')\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.resnet(images)\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.permute(0, 2, 3, 1)  # (B, 14, 14, 2048)\n",
    "        out = out.view(out.size(0), -1, out.size(-1))  # (B, num_pixels, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "# --- Decoder ---\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim=2048, dropout=0.5,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        else:\n",
    "            self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.init_h = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.init_c = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(embed_size + encoder_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, captions, caplens):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)\n",
    "        caplens, sort_ind = caplens.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        captions = captions[sort_ind]\n",
    "        embeddings = self.embedding(captions)\n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        decode_lengths = (caplens - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(encoder_out.device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            awe = encoder_out[:batch_size_t].mean(dim=1)\n",
    "            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], awe], dim=1)\n",
    "            h, c = self.lstm(input_lstm, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "\n",
    "        return predictions, captions, decode_lengths, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39705b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:40.167698Z",
     "iopub.status.busy": "2025-06-05T13:42:40.167366Z",
     "iopub.status.idle": "2025-06-05T13:42:40.346756Z",
     "shell.execute_reply": "2025-06-05T13:42:40.346124Z"
    },
    "papermill": {
     "duration": 0.184882,
     "end_time": "2025-06-05T13:42:40.348172",
     "exception": false,
     "start_time": "2025-06-05T13:42:40.163290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.h = h5py.File(os.path.join(data_folder, f\"{split}_images_{data_name}.hdf5\"), 'r')\n",
    "        self.imgs = self.h['images']\n",
    "        self.cpi = self.h.attrs['captions_per_image']\n",
    "\n",
    "        with open(os.path.join(data_folder, f\"{split}_captions_{data_name}.json\"), 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "        with open(os.path.join(data_folder, f\"{split}_caplength_{data_name}.json\"), 'r') as j:\n",
    "            self.caplens = json.load(j)\n",
    "\n",
    "        self.dataset_size = len(self.captions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return img, caption, caplen\n",
    "        else:\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)]\n",
    "            )\n",
    "            return img, caption, caplen, all_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01bf043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:40.356463Z",
     "iopub.status.busy": "2025-06-05T13:42:40.356156Z",
     "iopub.status.idle": "2025-06-05T13:43:06.801283Z",
     "shell.execute_reply": "2025-06-05T13:43:06.800403Z"
    },
    "papermill": {
     "duration": 26.450957,
     "end_time": "2025-06-05T13:43:06.802899",
     "exception": false,
     "start_time": "2025-06-05T13:42:40.351942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GloVe loaded. Coverage: 8577\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os, json\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "aspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\n",
    "data_folder = \"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset\"\n",
    "word_map_path = os.path.join(data_folder, \"wordmap_all.json\")\n",
    "glove_path = \"/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\"\n",
    "pretrained_checkpoint = \"/kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth\"\n",
    "output_dir = \"/kaggle/working/fine-tuned-models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "with open(word_map_path, \"r\") as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "vocab_size = len(word_map)\n",
    "embed_size = 300\n",
    "hidden_size = 512\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "patience = 3\n",
    "learning_rate = 1e-4\n",
    "grad_clip = 5.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transform ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- GloVe Embedding Loader ---\n",
    "def load_glove_embeddings(glove_path, word_map, embedding_dim=300):\n",
    "    import numpy as np\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_map), embedding_dim)).astype(np.float32)\n",
    "    for word, idx in word_map.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "    print(\"✅ GloVe loaded. Coverage:\", sum([1 for w in word_map if w in embeddings_index]))\n",
    "    return torch.tensor(embedding_matrix)\n",
    "\n",
    "embedding_matrix = load_glove_embeddings(glove_path, word_map, embed_size)\n",
    "\n",
    "# --- Load from pretrained checkpoint ---\n",
    "def load_pretrained_model(embed_matrix, ckpt_path):\n",
    "    encoder = EncoderCNN().to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        pretrained_embeddings=embed_matrix\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Load encoder state dict normally (semua layer seharusnya match)\n",
    "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "\n",
    "    # Load decoder state dict *selectively*\n",
    "    decoder_state = checkpoint[\"decoder\"]\n",
    "    model_state = decoder.state_dict()\n",
    "\n",
    "    # Filter matching keys\n",
    "    filtered_state = {k: v for k, v in decoder_state.items() if k in model_state and v.size() == model_state[k].size()}\n",
    "    model_state.update(filtered_state)\n",
    "    decoder.load_state_dict(model_state)\n",
    "\n",
    "    print(f\"✅ Partially loaded decoder from {ckpt_path} ({len(filtered_state)} layers matched)\")\n",
    "    return encoder, decoder\n",
    "\n",
    "# --- Training Utilities ---\n",
    "def train_epoch(loader, encoder, decoder, criterion, enc_opt, dec_opt, device):\n",
    "    encoder.train(); decoder.train(); total_loss = 0\n",
    "    for imgs, caps, caplens in tqdm(loader, desc=\"🔥 Training\", leave=False):\n",
    "        imgs, caps, caplens = imgs.to(device), caps.to(device), caplens.to(device)\n",
    "        enc_out = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, _ = decoder(enc_out, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        scores_packed = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets_packed = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        loss = criterion(scores_packed, targets_packed)\n",
    "        dec_opt.zero_grad(); enc_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
    "        dec_opt.step(); enc_opt.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(loader, encoder, decoder, criterion, device):\n",
    "    encoder.eval(); decoder.eval(); total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, caps, caplens, _ in tqdm(loader, desc=\"🔍 Evaluating\", leave=False):\n",
    "            imgs, caps, caplens = imgs.to(device), caps.to(device), caplens.to(device)\n",
    "            enc_out = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, _ = decoder(enc_out, caps, caplens)\n",
    "            targets = caps_sorted[:, 1:]\n",
    "            scores_packed = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "            targets_packed = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "            loss = criterion(scores_packed, targets_packed)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- Save checkpoint ---\n",
    "def save_checkpoint(aspect, epoch, encoder, decoder, enc_opt, dec_opt, val_loss, previous_ckpt_path=None):\n",
    "    ckpt_path = os.path.join(output_dir, f\"{aspect}_best.pth\")\n",
    "    \n",
    "    # Hapus checkpoint lama jika ada\n",
    "    if previous_ckpt_path and os.path.exists(previous_ckpt_path):\n",
    "        os.remove(previous_ckpt_path)\n",
    "        print(f\"🧹 Removed previous checkpoint: {previous_ckpt_path}\")\n",
    "\n",
    "    # Simpan checkpoint baru\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"encoder\": encoder.state_dict(),\n",
    "        \"decoder\": decoder.state_dict(),\n",
    "        \"encoder_optimizer\": enc_opt.state_dict(),\n",
    "        \"decoder_optimizer\": dec_opt.state_dict()\n",
    "    }\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(f\"💾 Saved: {ckpt_path}\")\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8d4319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:43:06.811567Z",
     "iopub.status.busy": "2025-06-05T13:43:06.811337Z",
     "iopub.status.idle": "2025-06-05T15:42:02.219809Z",
     "shell.execute_reply": "2025-06-05T15:42:02.218812Z"
    },
    "papermill": {
     "duration": 7135.414029,
     "end_time": "2025-06-05T15:42:02.220999",
     "exception": false,
     "start_time": "2025-06-05T13:43:06.806970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Fine-tuning for aspect: GENERAL_IMPRESSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
      "100%|██████████| 171M/171M [00:00<00:00, 185MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.7729 | 🔍 Val: 5.5960\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.6441 | 🔍 Val: 5.3103\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.3401 | 🔍 Val: 5.1401\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.1172 | 🔍 Val: 5.0258\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.9266 | 🔍 Val: 4.9477\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.7616 | 🔍 Val: 4.8924\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.6072 | 🔍 Val: 4.8504\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.4608 | 🔍 Val: 4.8441\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.3325 | 🔍 Val: 4.8148\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.2103 | 🔍 Val: 4.8026\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0937 | 🔍 Val: 4.7985\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/general_impression_best.pth\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.9807 | 🔍 Val: 4.8094\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.8742 | 🔍 Val: 4.8042\n",
      "⚠️ No improvement (2/3)\n",
      "\n",
      "🔁 Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.7719 | 🔍 Val: 4.8031\n",
      "⚠️ No improvement (3/3)\n",
      "🛑 Early stopping triggered.\n",
      "\n",
      "📌 Fine-tuning for aspect: SUBJECT\n",
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 7.3985 | 🔍 Val: 5.9497\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.9123 | 🔍 Val: 5.7361\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.6206 | 🔍 Val: 5.6022\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.3723 | 🔍 Val: 5.5094\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.1399 | 🔍 Val: 5.4439\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.9272 | 🔍 Val: 5.4093\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.7343 | 🔍 Val: 5.3854\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.5499 | 🔍 Val: 5.3744\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.3817 | 🔍 Val: 5.3706\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.2240 | 🔍 Val: 5.3612\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/subject_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0739 | 🔍 Val: 5.3973\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.9360 | 🔍 Val: 5.3943\n",
      "⚠️ No improvement (2/3)\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.7987 | 🔍 Val: 5.4078\n",
      "⚠️ No improvement (3/3)\n",
      "🛑 Early stopping triggered.\n",
      "\n",
      "📌 Fine-tuning for aspect: USE_OF_CAMERA\n",
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 9.0442 | 🔍 Val: 8.6800\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 8.3101 | 🔍 Val: 7.7100\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 7.1742 | 🔍 Val: 6.5910\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.4245 | 🔍 Val: 6.1587\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.0964 | 🔍 Val: 6.0040\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.9303 | 🔍 Val: 5.9179\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.7794 | 🔍 Val: 5.8607\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.6356 | 🔍 Val: 5.8153\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.4823 | 🔍 Val: 5.7819\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.3261 | 🔍 Val: 5.7495\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.1742 | 🔍 Val: 5.7202\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.0260 | 🔍 Val: 5.7024\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.8766 | 🔍 Val: 5.6868\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.7259 | 🔍 Val: 5.6675\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.5857 | 🔍 Val: 5.6643\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.4362 | 🔍 Val: 5.6541\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.3023 | 🔍 Val: 5.6524\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.1682 | 🔍 Val: 5.6555\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0513 | 🔍 Val: 5.6507\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/use_of_camera_best.pth\n",
      "\n",
      "🔁 Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.9318 | 🔍 Val: 5.6576\n",
      "⚠️ No improvement (1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for aspect in aspects:\n",
    "    print(f\"\\n📌 Fine-tuning for aspect: {aspect.upper()}\")\n",
    "    train_loader = DataLoader(\n",
    "        CaptionDataset(data_folder, aspect, split='train', transform=transform),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        CaptionDataset(data_folder, aspect, split='val', transform=transform),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    encoder, decoder = load_pretrained_model(embedding_matrix, pretrained_checkpoint)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    enc_opt = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=learning_rate)\n",
    "    dec_opt = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    best_ckpt_path = None\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n🔁 Epoch {epoch}/{num_epochs}\")\n",
    "        train_loss = train_epoch(train_loader, encoder, decoder, criterion, enc_opt, dec_opt, device)\n",
    "        val_loss = evaluate(val_loader, encoder, decoder, criterion, device)\n",
    "        print(f\"✅ Train: {train_loss:.4f} | 🔍 Val: {val_loss:.4f}\")\n",
    "    \n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_ckpt_path = save_checkpoint(aspect, epoch, encoder, decoder, enc_opt, dec_opt, val_loss, best_ckpt_path)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"⚠️ No improvement ({no_improve}/{patience})\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"🛑 Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092c9e3",
   "metadata": {
    "papermill": {
     "duration": 0.413405,
     "end_time": "2025-06-05T15:42:02.988899",
     "exception": false,
     "start_time": "2025-06-05T15:42:02.575494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60bf07f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:03.687562Z",
     "iopub.status.busy": "2025-06-05T15:42:03.686840Z",
     "iopub.status.idle": "2025-06-05T15:42:03.692917Z",
     "shell.execute_reply": "2025-06-05T15:42:03.692328Z"
    },
    "papermill": {
     "duration": 0.355578,
     "end_time": "2025-06-05T15:42:03.693947",
     "exception": false,
     "start_time": "2025-06-05T15:42:03.338369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# fine_tune_ckpt = \"/kaggle/input/single-aspects-part-2/pytorch/default/1/fine-tuned-models\"\n",
    "# aspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\n",
    "\n",
    "# def caption_image_beam_search(encoder, decoder, image, word_map, beam_size=5, max_len=25):\n",
    "#     k = beam_size\n",
    "#     vocab_size = len(word_map)\n",
    "#     rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "#     encoder_out = encoder(image.unsqueeze(0))  # (1, num_pixels, encoder_dim)\n",
    "#     encoder_dim = encoder_out.size(-1)\n",
    "#     encoder_out = encoder_out.expand(k, -1, encoder_dim)\n",
    "\n",
    "#     k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n",
    "#     seqs = k_prev_words\n",
    "#     top_k_scores = torch.zeros(k, 1).to(device)\n",
    "\n",
    "#     complete_seqs = []\n",
    "#     complete_seqs_scores = []\n",
    "\n",
    "#     h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "#     step = 1\n",
    "#     while True:\n",
    "#         embeddings = decoder.embedding(k_prev_words).squeeze(1)\n",
    "#         awe = encoder_out.mean(dim=1)  # mean attention\n",
    "#         input_lstm = torch.cat([embeddings, awe], dim=1)\n",
    "#         h, c = decoder.lstm(input_lstm, (h, c))\n",
    "#         scores = decoder.fc(h)\n",
    "#         scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "#         scores = top_k_scores.expand_as(scores) + scores\n",
    "#         if step == 1:\n",
    "#             top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "#         else:\n",
    "#             top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
    "\n",
    "#         prev_word_inds = top_k_words // vocab_size\n",
    "#         next_word_inds = top_k_words % vocab_size\n",
    "\n",
    "#         seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "\n",
    "#         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "#         complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "#         if len(complete_inds) > 0:\n",
    "#             complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "#             complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "\n",
    "#         k -= len(complete_inds)\n",
    "#         if k == 0 or step > max_len:\n",
    "#             break\n",
    "\n",
    "#         seqs = seqs[incomplete_inds]\n",
    "#         h = h[prev_word_inds[incomplete_inds]]\n",
    "#         c = c[prev_word_inds[incomplete_inds]]\n",
    "#         encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "#         top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "#         k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "#         step += 1\n",
    "\n",
    "#     if len(complete_seqs) == 0:\n",
    "#     # fallback to the current best in seqs\n",
    "#         best_seq = seqs[0].tolist()\n",
    "#     else:\n",
    "#         i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "#         best_seq = complete_seqs[i]\n",
    "\n",
    "#     decoded = [rev_word_map[idx] for idx in best_seq if idx not in {word_map['<start>'], word_map['<pad>'], word_map['<end>']}]\n",
    "#     return decoded\n",
    "\n",
    "# # --- Evaluasi Semua Aspek ---\n",
    "# bleu_scores = {}\n",
    "\n",
    "# # Load word_map & reverse-nya\n",
    "# with open(word_map_path, \"r\") as f:\n",
    "#     word_map = json.load(f)\n",
    "# rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "# from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# results = []\n",
    "# chencherry = SmoothingFunction()\n",
    "\n",
    "# for aspect in aspects:\n",
    "#     print(f\"\\n📊 Evaluating aspect: {aspect.upper()}\")\n",
    "#     val_loader = DataLoader(\n",
    "#         CaptionDataset(data_folder, aspect, split='val', transform=transform),\n",
    "#         batch_size=1, shuffle=False\n",
    "#     )\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#     encoder.eval(); decoder.eval()\n",
    "\n",
    "#     references, hypotheses = [], []\n",
    "#     total_unk = 0\n",
    "#     total_words = 0\n",
    "\n",
    "#     for img, _, _, all_caps in tqdm(val_loader, desc=f\"📝 Beam Decode ({aspect})\"):\n",
    "#         img = img.squeeze(0).to(device)\n",
    "#         gen_caption = caption_image_beam_search(encoder, decoder, img, word_map, beam_size=5, max_len=25)\n",
    "#         hypotheses.append(gen_caption)\n",
    "#         total_words += len(gen_caption)\n",
    "#         total_unk += gen_caption.count('<unk>')\n",
    "\n",
    "#         caps = all_caps[0].tolist()\n",
    "#         refs = [[rev_word_map[i] for i in cap if i not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n",
    "#         references.append(refs)\n",
    "\n",
    "#     results.append({\n",
    "#         \"Aspect\": aspect,\n",
    "#         \"BLEU-1\": corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-2\": corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-3\": corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-4\": corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1),\n",
    "#         \"UNK Ratio\": total_unk / total_words if total_words > 0 else 0\n",
    "#     })\n",
    "\n",
    "# import pandas as pd\n",
    "# df_results = pd.DataFrame(results)\n",
    "# print(\"\\n📈 BLEU-n & UNK Summary\")\n",
    "# print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6bfa8",
   "metadata": {
    "papermill": {
     "duration": 0.428053,
     "end_time": "2025-06-05T15:42:04.472743",
     "exception": false,
     "start_time": "2025-06-05T15:42:04.044690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6829a098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:05.200807Z",
     "iopub.status.busy": "2025-06-05T15:42:05.200510Z",
     "iopub.status.idle": "2025-06-05T15:42:05.204751Z",
     "shell.execute_reply": "2025-06-05T15:42:05.204027Z"
    },
    "papermill": {
     "duration": 0.359555,
     "end_time": "2025-06-05T15:42:05.205890",
     "exception": false,
     "start_time": "2025-06-05T15:42:04.846335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_random_predictions(aspects, k=10):\n",
    "#     for aspect in aspects:\n",
    "#         print(f\"\\n🎨 Aspect: {aspect.upper()}\")\n",
    "#         dataset = CaptionDataset(data_folder, aspect, split='test', transform=transform)\n",
    "#         indices = random.sample(range(len(dataset)), k)\n",
    "#         subset = [dataset[i] for i in indices]\n",
    "\n",
    "#         encoder = EncoderCNN().to(device)\n",
    "#         decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "#         ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#         encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#         decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#         encoder.eval(); decoder.eval()\n",
    "\n",
    "#         for i, (img, _, _, all_caps) in enumerate(subset):\n",
    "#             img_tensor = img.to(device)\n",
    "#             gen_caption = caption_image_beam_search(encoder, decoder, img_tensor, word_map, beam_size=5, max_len=25)\n",
    "#             caps = all_caps.squeeze(0).tolist()  # shape: (5, max_len) -> list of list\n",
    "#             # references = [[rev_word_map[idx] for idx in cap if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n",
    "#             references = [\n",
    "#                 [rev_word_map[idx] for idx in cap if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "#                 for cap in caps\n",
    "#             ]\n",
    "\n",
    "#             print(f\"\\n📸 Sample {i+1}\")\n",
    "#             print(\"🔹 Predicted :\", \" \".join(gen_caption))\n",
    "#             print(\"🔸 References:\")\n",
    "#             for r in references[:3]:\n",
    "#                 print(\"  -\", \" \".join(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20eb945b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:05.977839Z",
     "iopub.status.busy": "2025-06-05T15:42:05.977073Z",
     "iopub.status.idle": "2025-06-05T15:42:05.981361Z",
     "shell.execute_reply": "2025-06-05T15:42:05.980512Z"
    },
    "papermill": {
     "duration": 0.422217,
     "end_time": "2025-06-05T15:42:05.982828",
     "exception": false,
     "start_time": "2025-06-05T15:42:05.560611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show_random_predictions([\"general_impression\", \"subject\", \"use_of_camera\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ede8a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:06.709277Z",
     "iopub.status.busy": "2025-06-05T15:42:06.708698Z",
     "iopub.status.idle": "2025-06-05T15:42:06.713809Z",
     "shell.execute_reply": "2025-06-05T15:42:06.713241Z"
    },
    "papermill": {
     "duration": 0.367575,
     "end_time": "2025-06-05T15:42:06.714898",
     "exception": false,
     "start_time": "2025-06-05T15:42:06.347323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict_image_caption(image_path, aspect, ckpt_dir, word_map_path, embed_matrix, beam_size=5, max_len=25):\n",
    "#     \"\"\"\n",
    "#     Generate a caption for a given image and aesthetic aspect.\n",
    "#     \"\"\"\n",
    "#     with open(word_map_path, \"r\") as f:\n",
    "#         word_map = json.load(f)\n",
    "\n",
    "#     vocab_size = len(word_map)\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(\n",
    "#         embed_size=embed_matrix.shape[1],\n",
    "#         hidden_size=512,\n",
    "#         vocab_size=vocab_size,\n",
    "#         pretrained_embeddings=embed_matrix,\n",
    "#         freeze_embeddings=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(ckpt_dir, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#     encoder.eval()\n",
    "#     decoder.eval()\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).to(device)\n",
    "\n",
    "#     caption_ids = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "#     caption_text = \" \".join(caption_ids)\n",
    "#     print(f\"📷 {aspect.upper()} - Caption:\")\n",
    "#     print(f\"📝 {caption_text}\")\n",
    "#     return caption_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3714ff85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:07.508218Z",
     "iopub.status.busy": "2025-06-05T15:42:07.507424Z",
     "iopub.status.idle": "2025-06-05T15:42:07.511165Z",
     "shell.execute_reply": "2025-06-05T15:42:07.510641Z"
    },
    "papermill": {
     "duration": 0.434389,
     "end_time": "2025-06-05T15:42:07.512361",
     "exception": false,
     "start_time": "2025-06-05T15:42:07.077972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_image_caption(\n",
    "#     image_path=\"/kaggle/input/dpchallenge-images-food-gallery/images/1000368.jpg\",  # atau path gambar lokal\n",
    "#     aspect=\"general_impression\",\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects-part-2/pytorch/default/1/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix,  # variabel yang kamu sudah punya sebelumnya\n",
    "#     beam_size=5,\n",
    "#     max_len=25\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489551cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:08.250325Z",
     "iopub.status.busy": "2025-06-05T15:42:08.250045Z",
     "iopub.status.idle": "2025-06-05T15:42:08.253610Z",
     "shell.execute_reply": "2025-06-05T15:42:08.253041Z"
    },
    "papermill": {
     "duration": 0.355642,
     "end_time": "2025-06-05T15:42:08.254704",
     "exception": false,
     "start_time": "2025-06-05T15:42:07.899062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix):\n",
    "#     with open(word_map_path, 'r') as j:\n",
    "#         word_map = json.load(j)\n",
    "#     vocab_size = len(word_map)\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(\n",
    "#         embed_size=embed_size,\n",
    "#         hidden_size=hidden_size,\n",
    "#         vocab_size=vocab_size,\n",
    "#         pretrained_embeddings=embed_matrix,\n",
    "#         freeze_embeddings=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(ckpt_dir, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "\n",
    "#     return encoder, decoder, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ee7abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:09.016647Z",
     "iopub.status.busy": "2025-06-05T15:42:09.016021Z",
     "iopub.status.idle": "2025-06-05T15:42:09.020217Z",
     "shell.execute_reply": "2025-06-05T15:42:09.019701Z"
    },
    "papermill": {
     "duration": 0.417371,
     "end_time": "2025-06-05T15:42:09.021461",
     "exception": false,
     "start_time": "2025-06-05T15:42:08.604090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def caption_image_beam_search(encoder, decoder, image, word_map, beam_size=5, max_len=25):\n",
    "#     k = beam_size\n",
    "#     vocab_size = len(word_map)\n",
    "#     rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "#     encoder_out = encoder(image.unsqueeze(0))\n",
    "#     encoder_dim = encoder_out.size(-1)\n",
    "#     encoder_out = encoder_out.expand(k, -1, encoder_dim)\n",
    "\n",
    "#     k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n",
    "#     seqs = k_prev_words\n",
    "#     top_k_scores = torch.zeros(k, 1).to(device)\n",
    "\n",
    "#     complete_seqs = []\n",
    "#     complete_seqs_scores = []\n",
    "\n",
    "#     h, c = decoder.init_hidden_state(encoder_out)\n",
    "#     step = 1\n",
    "#     while True:\n",
    "#         embeddings = decoder.embedding(k_prev_words).squeeze(1)\n",
    "#         awe = encoder_out.mean(dim=1)\n",
    "#         input_lstm = torch.cat([embeddings, awe], dim=1)\n",
    "#         h, c = decoder.lstm(input_lstm, (h, c))\n",
    "#         scores = decoder.fc(h)\n",
    "#         scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "#         scores = top_k_scores.expand_as(scores) + scores\n",
    "#         if step == 1:\n",
    "#             top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "#         else:\n",
    "#             top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
    "\n",
    "#         prev_word_inds = top_k_words // vocab_size\n",
    "#         next_word_inds = top_k_words % vocab_size\n",
    "#         seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "\n",
    "#         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "#         complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "#         if len(complete_inds) > 0:\n",
    "#             complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "#             complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "\n",
    "#         k -= len(complete_inds)\n",
    "#         if k == 0 or step > max_len:\n",
    "#             break\n",
    "\n",
    "#         seqs = seqs[incomplete_inds]\n",
    "#         h = h[prev_word_inds[incomplete_inds]]\n",
    "#         c = c[prev_word_inds[incomplete_inds]]\n",
    "#         encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "#         top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "#         k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "#         step += 1\n",
    "\n",
    "#     if len(complete_seqs_scores) == 0:\n",
    "#         return [\"<unk>\"]\n",
    "\n",
    "#     best_seq = complete_seqs[complete_seqs_scores.index(max(complete_seqs_scores))]\n",
    "#     return [rev_word_map[idx] for idx in best_seq if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e968d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:09.720422Z",
     "iopub.status.busy": "2025-06-05T15:42:09.719767Z",
     "iopub.status.idle": "2025-06-05T15:42:09.724085Z",
     "shell.execute_reply": "2025-06-05T15:42:09.723295Z"
    },
    "papermill": {
     "duration": 0.359894,
     "end_time": "2025-06-05T15:42:09.725472",
     "exception": false,
     "start_time": "2025-06-05T15:42:09.365578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def predict_image_caption(image_path, aspect, ckpt_dir, word_map_path, embed_matrix, beam_size=5, max_len=25):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).to(device)\n",
    "\n",
    "#     encoder, decoder, word_map = load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix)\n",
    "#     caption = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "\n",
    "#     print(f\"📷 {os.path.basename(image_path)} — [{aspect.upper()}]\")\n",
    "#     print(f\"📝 Caption: {' '.join(caption)}\")\n",
    "\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"{aspect.upper()} — {' '.join(caption)}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6670507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:10.488182Z",
     "iopub.status.busy": "2025-06-05T15:42:10.487914Z",
     "iopub.status.idle": "2025-06-05T15:42:10.491953Z",
     "shell.execute_reply": "2025-06-05T15:42:10.491247Z"
    },
    "papermill": {
     "duration": 0.409971,
     "end_time": "2025-06-05T15:42:10.493146",
     "exception": false,
     "start_time": "2025-06-05T15:42:10.083175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def infer_visualize_random_images(aspect, dataset, ckpt_dir, word_map_path, embed_matrix,\n",
    "#                                    test_split=\"test\", beam_size=5, max_len=25, n_samples=10):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     test_images = [img['filename'] for img in dataset if img['split'] == test_split]\n",
    "#     selected_images = random.sample(test_images, n_samples)\n",
    "\n",
    "#     encoder, decoder, word_map = load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix)\n",
    "\n",
    "#     for idx, filename in enumerate(selected_images):\n",
    "#         image_path = os.path.join(\"/kaggle/input/dpchallenge-images-food-gallery/images\", filename)\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         image_tensor = transform(image).to(device)\n",
    "\n",
    "#         caption = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "#         plt.figure(figsize=(6, 4))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"[{aspect.upper()}] — {' '.join(caption)}\", fontsize=10)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39025093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:11.186057Z",
     "iopub.status.busy": "2025-06-05T15:42:11.185536Z",
     "iopub.status.idle": "2025-06-05T15:42:11.189281Z",
     "shell.execute_reply": "2025-06-05T15:42:11.188691Z"
    },
    "papermill": {
     "duration": 0.350448,
     "end_time": "2025-06-05T15:42:11.190399",
     "exception": false,
     "start_time": "2025-06-05T15:42:10.839951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# # aspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"general_impression\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects-part-2/pytorch/default/1/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe8688f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:11.966913Z",
     "iopub.status.busy": "2025-06-05T15:42:11.966638Z",
     "iopub.status.idle": "2025-06-05T15:42:11.969931Z",
     "shell.execute_reply": "2025-06-05T15:42:11.969415Z"
    },
    "papermill": {
     "duration": 0.43478,
     "end_time": "2025-06-05T15:42:11.970990",
     "exception": false,
     "start_time": "2025-06-05T15:42:11.536210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# # aspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"subject\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects-part-2/pytorch/default/1/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2792a05c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T15:42:12.672145Z",
     "iopub.status.busy": "2025-06-05T15:42:12.671884Z",
     "iopub.status.idle": "2025-06-05T15:42:12.675438Z",
     "shell.execute_reply": "2025-06-05T15:42:12.674871Z"
    },
    "papermill": {
     "duration": 0.359034,
     "end_time": "2025-06-05T15:42:12.676519",
     "exception": false,
     "start_time": "2025-06-05T15:42:12.317485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# # aspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"use_of_camera\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects-part-2/pytorch/default/1/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5504,
     "sourceId": 8240,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7591480,
     "sourceId": 12061127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7593368,
     "sourceId": 12070627,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 352589,
     "modelInstanceId": 331702,
     "sourceId": 405921,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7191.103792,
   "end_time": "2025-06-05T15:42:16.641454",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T13:42:25.537662",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
