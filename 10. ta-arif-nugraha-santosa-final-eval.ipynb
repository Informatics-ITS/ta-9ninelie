{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12061127,"sourceType":"datasetVersion","datasetId":7591480},{"sourceId":12070627,"sourceType":"datasetVersion","datasetId":7593368},{"sourceId":426045,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":346474,"modelId":367752},{"sourceId":426047,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":346728,"modelId":367998},{"sourceId":435105,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":354853,"modelId":376169}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install pycocoevalcap and dependencies\n!git clone https://github.com/salaniz/pycocoevalcap.git\n!pip install git+https://github.com/salaniz/pycocoevalcap\n!pip install nltk\n!python -m nltk.downloader punkt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:23:37.827285Z","iopub.execute_input":"2025-07-16T06:23:37.828229Z","iopub.status.idle":"2025-07-16T06:24:03.095863Z","shell.execute_reply.started":"2025-07-16T06:23:37.828197Z","shell.execute_reply":"2025-07-16T06:24:03.094756Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'pycocoevalcap' already exists and is not an empty directory.\nCollecting git+https://github.com/salaniz/pycocoevalcap\n  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-umeymzsa\n  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-umeymzsa\n  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\nRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\nBuilding wheels for collected packages: pycocoevalcap\n  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=14df605407bf39014bf38718471d60509887806ae0ab71eb033a62a16584f2fa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ly7zcn7c/wheels/d2/1f/44/6485e566f8ae3d42b56e7c05fd50a3bbb70a50b0e6e7c55212\nSuccessfully built pycocoevalcap\nInstalling collected packages: pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# CNN-LSTM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport os\nimport json\nfrom tqdm import tqdm\n\n# --- Encoder ---\nclass EncoderCNN(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(EncoderCNN, self).__init__()\n        self.enc_image_size = encoded_image_size\n        self.encoder_dim = 2048\n\n        resnet = models.resnet101(weights='DEFAULT')\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune()\n\n    def forward(self, images):\n        out = self.resnet(images)\n        out = self.adaptive_pool(out)\n        out = out.permute(0, 2, 3, 1)  # (B, 14, 14, 2048)\n        out = out.view(out.size(0), -1, out.size(-1))  # (B, num_pixels, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune\n\n# --- Decoder ---\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim=2048, dropout=0.5,\n                 pretrained_embeddings=None, freeze_embeddings=False):\n        super(DecoderRNN, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        if pretrained_embeddings is not None:\n            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n            self.embedding.weight.requires_grad = not freeze_embeddings\n        else:\n            self.embedding.weight.data.uniform_(-0.1, 0.1)\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.init_h = nn.Linear(encoder_dim, hidden_size)\n        self.init_c = nn.Linear(encoder_dim, hidden_size)\n        self.lstm = nn.LSTMCell(embed_size + encoder_dim, hidden_size)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, captions, caplens):\n        batch_size = encoder_out.size(0)\n        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)\n        caplens, sort_ind = caplens.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        captions = captions[sort_ind]\n        embeddings = self.embedding(captions)\n\n        h, c = self.init_hidden_state(encoder_out)\n        decode_lengths = (caplens - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(encoder_out.device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            awe = encoder_out[:batch_size_t].mean(dim=1)\n            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], awe], dim=1)\n            h, c = self.lstm(input_lstm, (h[:batch_size_t], c[:batch_size_t]))\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n\n        return predictions, captions, decode_lengths, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:24:03.097966Z","iopub.execute_input":"2025-07-16T06:24:03.098291Z","iopub.status.idle":"2025-07-16T06:24:03.114887Z","shell.execute_reply.started":"2025-07-16T06:24:03.098262Z","shell.execute_reply":"2025-07-16T06:24:03.114020Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset\nimport h5py\n\nclass CaptionDataset(Dataset):\n    def __init__(self, data_folder, data_name, split, transform=None):\n        self.split = split\n        self.transform = transform\n\n        self.h = h5py.File(os.path.join(data_folder, f\"{split}_images_{data_name}.hdf5\"), 'r')\n        self.imgs = self.h['images']\n        self.cpi = self.h.attrs['captions_per_image']\n\n        with open(os.path.join(data_folder, f\"{split}_captions_{data_name}.json\"), 'r') as j:\n            self.captions = json.load(j)\n        with open(os.path.join(data_folder, f\"{split}_caplength_{data_name}.json\"), 'r') as j:\n            self.caplens = json.load(j)\n\n        self.dataset_size = len(self.captions)\n\n    def __getitem__(self, i):\n        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        caption = torch.LongTensor(self.captions[i])\n        caplen = torch.LongTensor([self.caplens[i]])\n\n        if self.split == 'train':\n            return img, caption, caplen\n        else:\n            all_captions = torch.LongTensor(\n                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)]\n            )\n            return img, caption, caplen, all_captions\n\n    def __len__(self):\n        return self.dataset_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:24:03.116248Z","iopub.execute_input":"2025-07-16T06:24:03.116582Z","iopub.status.idle":"2025-07-16T06:24:03.134728Z","shell.execute_reply.started":"2025-07-16T06:24:03.116555Z","shell.execute_reply":"2025-07-16T06:24:03.133880Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport os, json\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n# --- Konfigurasi ---\ndata_folder = \"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset\"\nword_map_path = os.path.join(data_folder, \"wordmap_all.json\")\n\n# --- Hyperparameters ---\nwith open(word_map_path, \"r\") as j:\n    word_map = json.load(j)\n\nvocab_size = len(word_map)\nembed_size = 300\nhidden_size = 512\nbatch_size = 64\nnum_epochs = 20\npatience = 3\nlearning_rate = 1e-4\ngrad_clip = 5.0\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Transform ---\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:22:03.309795Z","iopub.execute_input":"2025-07-16T06:22:03.310125Z","iopub.status.idle":"2025-07-16T06:22:03.329143Z","shell.execute_reply.started":"2025-07-16T06:22:03.310099Z","shell.execute_reply":"2025-07-16T06:22:03.328174Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix):\n    with open(word_map_path, 'r') as j:\n        word_map = json.load(j)\n    vocab_size = len(word_map)\n\n    encoder = EncoderCNN().to(device)\n    decoder = DecoderRNN(\n        embed_size=embed_size,\n        hidden_size=hidden_size,\n        vocab_size=vocab_size,\n        pretrained_embeddings=embed_matrix,\n        freeze_embeddings=True\n    ).to(device)\n\n    ckpt = torch.load(os.path.join(ckpt_dir, f\"{aspect}_best.pth\"), map_location=device)\n    encoder.load_state_dict(ckpt[\"encoder\"])\n    decoder.load_state_dict(ckpt[\"decoder\"])\n\n    return encoder, decoder, word_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:24:04.862655Z","iopub.execute_input":"2025-07-16T06:24:04.863346Z","iopub.status.idle":"2025-07-16T06:24:04.868790Z","shell.execute_reply.started":"2025-07-16T06:24:04.863318Z","shell.execute_reply":"2025-07-16T06:24:04.867981Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def caption_image_beam_search(encoder, decoder, image, word_map, beam_size=5, max_len=25, unk_penalty_weight=0.5):\n    k = beam_size\n    vocab_size = len(word_map)\n    rev_word_map = {v: k for k, v in word_map.items()}\n    unk_idx = word_map['<unk>']\n\n    encoder_out = encoder(image.unsqueeze(0))  # (1, num_pixels, encoder_dim)\n    encoder_dim = encoder_out.size(-1)\n    encoder_out = encoder_out.expand(k, -1, encoder_dim)\n\n    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n    seqs = k_prev_words\n    top_k_scores = torch.zeros(k, 1).to(device)\n\n    complete_seqs = []\n    complete_seqs_scores = []\n\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    step = 1\n    while True:\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)\n        awe = encoder_out.mean(dim=1)  # mean attention\n        input_lstm = torch.cat([embeddings, awe], dim=1)\n        h, c = decoder.lstm(input_lstm, (h, c))\n        scores = decoder.fc(h)\n        scores = F.log_softmax(scores, dim=1)\n\n        scores = top_k_scores.expand_as(scores) + scores\n        if step == 1:\n            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n        else:\n            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n\n        prev_word_inds = top_k_words // vocab_size\n        next_word_inds = top_k_words % vocab_size\n\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n\n        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n        if len(complete_inds) > 0:\n            complete_seqs.extend(seqs[complete_inds].tolist())\n            complete_seqs_scores.extend(top_k_scores[complete_inds])\n\n        k -= len(complete_inds)\n        if k == 0 or step > max_len:\n            break\n\n        seqs = seqs[incomplete_inds]\n        h = h[prev_word_inds[incomplete_inds]]\n        c = c[prev_word_inds[incomplete_inds]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n        step += 1\n\n    if len(complete_seqs) == 0:\n        best_seq = seqs[0].tolist()\n    else:\n        # Penalize scores by UNK count\n        penalized_scores = [\n            score - unk_penalty_weight * seq.count(unk_idx)\n            for seq, score in zip(complete_seqs, complete_seqs_scores)\n        ]\n        best_index = penalized_scores.index(max(penalized_scores))\n        best_seq = complete_seqs[best_index]\n\n    decoded = [rev_word_map[idx] for idx in best_seq if idx not in {word_map['<start>'], word_map['<pad>'], word_map['<end>']}]\n    return decoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T03:25:06.421814Z","iopub.execute_input":"2025-06-15T03:25:06.422494Z","iopub.status.idle":"2025-06-15T03:25:06.433054Z","shell.execute_reply.started":"2025-06-15T03:25:06.422473Z","shell.execute_reply":"2025-06-15T03:25:06.432104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_folder = \"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset\"\n\nvocab_size = len(word_map)\nembed_size = 300\nhidden_size = 512\nbatch_size = 64\nnum_epochs = 20\npatience = 3\nlearning_rate = 1e-4\ngrad_clip = 5.0\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:24:10.217702Z","iopub.execute_input":"2025-07-16T06:24:10.218051Z","iopub.status.idle":"2025-07-16T06:24:10.223109Z","shell.execute_reply.started":"2025-07-16T06:24:10.218028Z","shell.execute_reply":"2025-07-16T06:24:10.222271Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.spice.spice import Spice\n\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport pandas as pd\n\naspects = [\"color_light\", \"composition\", \"dof_and_focus\"]\nfine_tune_ckpt = \"/kaggle/input/single-aspects/pytorch/default/3/fine-tuned-models\"\n\n# Load word map\nwith open(word_map_path, \"r\") as f:\n    word_map = json.load(f)\nrev_word_map = {v: k for k, v in word_map.items()}\n\n# Smoother untuk BLEU\nchencherry = SmoothingFunction()\n\nresults = []\n\nfor aspect in aspects:\n    print(f\"\\nüìä Evaluating aspect: {aspect.upper()}\")\n    val_loader = DataLoader(\n        CaptionDataset(data_folder, aspect, split='val', transform=transform),\n        batch_size=1, shuffle=False\n    )\n\n    encoder = EncoderCNN().to(device)\n    decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n\n    ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n    encoder.load_state_dict(ckpt[\"encoder\"])\n    decoder.load_state_dict(ckpt[\"decoder\"])\n    encoder.eval(); decoder.eval()\n\n    references_bleu, hypotheses_bleu = [], []\n    gts, res = {}, {}\n    total_unk, total_words = 0, 0\n\n    for i, (img, _, _, all_caps) in enumerate(tqdm(val_loader, desc=f\"üìù Beam Decode ({aspect})\")):\n        img = img.squeeze(0).to(device)\n        gen_caption = caption_image_beam_search(encoder, decoder, img, word_map, beam_size=5, max_len=25)\n        total_words += len(gen_caption)\n        total_unk += gen_caption.count('<unk>')\n\n        # Prepare for BLEU\n        hypotheses_bleu.append(gen_caption)\n        caps = all_caps[0].tolist()\n        refs = [[rev_word_map[i] for i in cap if i not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n        references_bleu.append(refs)\n\n        # Prepare for COCO-style metrics\n        res[i] = [{'caption': ' '.join(gen_caption)}]\n        gts[i] = [{'caption': ' '.join(ref)} for ref in refs]\n\n    # BLEU Scores\n    bleu1 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n    bleu2 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1)\n    bleu3 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1)\n    bleu4 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1)\n\n    # Tokenizer\n    tokenizer = PTBTokenizer()\n    gts_tok = tokenizer.tokenize(gts)\n    res_tok = tokenizer.tokenize(res)\n\n    # ROUGE-L\n    rouge_scorer = Rouge()\n    rouge_score, _ = rouge_scorer.compute_score(gts_tok, res_tok)\n\n    # METEOR\n    meteor_scorer = Meteor()\n    meteor_score, _ = meteor_scorer.compute_score(gts_tok, res_tok)\n\n    # CIDEr\n    cider_scorer = Cider()\n    cider_score, _ = cider_scorer.compute_score(gts_tok, res_tok)\n\n    # SPICE\n    try:\n        spice_scorer = Spice()\n        spice_score, _ = spice_scorer.compute_score(gts_tok, res_tok)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Skipping SPICE for {aspect} due to error: {e}\")\n        spice_score = -1  # bisa juga None\n\n    results.append({\n        \"Aspect\": aspect,\n        \"BLEU-1\": bleu1,\n        \"BLEU-2\": bleu2,\n        \"BLEU-3\": bleu3,\n        \"BLEU-4\": bleu4,\n        \"ROUGE-L\": rouge_score,\n        \"METEOR\": meteor_score,\n        \"CIDEr\": cider_score,\n        \"SPICE\": spice_score,\n        \"UNK Ratio\": total_unk / total_words if total_words > 0 else 0\n    })\n\n# Hasil akhir\ndf_results = pd.DataFrame(results)\nprint(\"\\nüìà Full Evaluation Results:\")\ndisplay(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T03:52:09.457263Z","iopub.execute_input":"2025-06-15T03:52:09.458072Z","iopub.status.idle":"2025-06-15T04:00:12.090546Z","shell.execute_reply.started":"2025-06-15T03:52:09.458047Z","shell.execute_reply":"2025-06-15T04:00:12.089899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.spice.spice import Spice\n\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport pandas as pd\n\naspects = [\"general_impression\", \"subject\", \"use_of_camera\"]\nfine_tune_ckpt = \"/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models\"\n\n# Load word map\nwith open(word_map_path, \"r\") as f:\n    word_map = json.load(f)\nrev_word_map = {v: k for k, v in word_map.items()}\n\n# Smoother untuk BLEU\nchencherry = SmoothingFunction()\n\nresults = []\n\nfor aspect in aspects:\n    print(f\"\\nüìä Evaluating aspect: {aspect.upper()}\")\n    val_loader = DataLoader(\n        CaptionDataset(data_folder, aspect, split='val', transform=transform),\n        batch_size=1, shuffle=False\n    )\n\n    encoder = EncoderCNN().to(device)\n    decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n\n    ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n    encoder.load_state_dict(ckpt[\"encoder\"])\n    decoder.load_state_dict(ckpt[\"decoder\"])\n    encoder.eval(); decoder.eval()\n\n    references_bleu, hypotheses_bleu = [], []\n    gts, res = {}, {}\n    total_unk, total_words = 0, 0\n\n    for i, (img, _, _, all_caps) in enumerate(tqdm(val_loader, desc=f\"üìù Beam Decode ({aspect})\")):\n        img = img.squeeze(0).to(device)\n        gen_caption = caption_image_beam_search(encoder, decoder, img, word_map, beam_size=5, max_len=25)\n        total_words += len(gen_caption)\n        total_unk += gen_caption.count('<unk>')\n\n        # Prepare for BLEU\n        hypotheses_bleu.append(gen_caption)\n        caps = all_caps[0].tolist()\n        refs = [[rev_word_map[i] for i in cap if i not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n        references_bleu.append(refs)\n\n        # Prepare for COCO-style metrics\n        res[i] = [{'caption': ' '.join(gen_caption)}]\n        gts[i] = [{'caption': ' '.join(ref)} for ref in refs]\n\n    # BLEU Scores\n    bleu1 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n    bleu2 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1)\n    bleu3 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1)\n    bleu4 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1)\n\n    # Tokenizer\n    tokenizer = PTBTokenizer()\n    gts_tok = tokenizer.tokenize(gts)\n    res_tok = tokenizer.tokenize(res)\n\n    # ROUGE-L\n    rouge_scorer = Rouge()\n    rouge_score, _ = rouge_scorer.compute_score(gts_tok, res_tok)\n\n    # METEOR\n    meteor_scorer = Meteor()\n    meteor_score, _ = meteor_scorer.compute_score(gts_tok, res_tok)\n\n    # CIDEr\n    cider_scorer = Cider()\n    cider_score, _ = cider_scorer.compute_score(gts_tok, res_tok)\n\n    # SPICE\n    try:\n        spice_scorer = Spice()\n        spice_score, _ = spice_scorer.compute_score(gts_tok, res_tok)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Skipping SPICE for {aspect} due to error: {e}\")\n        spice_score = -1  # bisa juga None\n\n    results.append({\n        \"Aspect\": aspect,\n        \"BLEU-1\": bleu1,\n        \"BLEU-2\": bleu2,\n        \"BLEU-3\": bleu3,\n        \"BLEU-4\": bleu4,\n        \"ROUGE-L\": rouge_score,\n        \"METEOR\": meteor_score,\n        \"CIDEr\": cider_score,\n        \"SPICE\": spice_score,\n        \"UNK Ratio\": total_unk / total_words if total_words > 0 else 0\n    })\n\n# Hasil akhir\ndf_results = pd.DataFrame(results)\nprint(\"\\nüìà Full Evaluation Results:\")\ndisplay(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:02:36.866459Z","iopub.execute_input":"2025-06-15T04:02:36.867053Z","iopub.status.idle":"2025-06-15T04:08:18.825887Z","shell.execute_reply.started":"2025-06-15T04:02:36.867030Z","shell.execute_reply":"2025-06-15T04:08:18.825005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.spice.spice import Spice\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nimport pandas as pd\n\n# Semua aspek & path ckpt-nya\naspect_ckpt_map = {\n    \"general_impression\": \"/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models\",\n    \"subject\": \"/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models\",\n    \"use_of_camera\": \"/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models\",\n    \"color_light\": \"/kaggle/input/single-aspects/pytorch/default/3/fine-tuned-models\",\n    \"composition\": \"/kaggle/input/single-aspects/pytorch/default/3/fine-tuned-models\",\n    \"dof_and_focus\": \"/kaggle/input/single-aspects/pytorch/default/3/fine-tuned-models\"\n}\n\n# Load word map\nwith open(word_map_path, \"r\") as f:\n    word_map = json.load(f)\nrev_word_map = {v: k for k, v in word_map.items()}\nchencherry = SmoothingFunction()\nresults = []\n\nfor aspect, ckpt_dir in aspect_ckpt_map.items():\n    print(f\"\\nüìä Evaluating aspect: {aspect.upper()}\")\n    val_loader = DataLoader(\n        CaptionDataset(data_folder, aspect, split='val', transform=transform),\n        batch_size=1, shuffle=False\n    )\n\n    encoder = EncoderCNN().to(device)\n    decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n\n    ckpt_path = os.path.join(ckpt_dir, f\"{aspect}_best.pth\")\n    ckpt = torch.load(ckpt_path, map_location=device)\n    encoder.load_state_dict(ckpt[\"encoder\"])\n    decoder.load_state_dict(ckpt[\"decoder\"])\n    encoder.eval(); decoder.eval()\n\n    references_bleu, hypotheses_bleu = [], []\n    gts, res = {}, {}\n    total_unk, total_words = 0, 0\n\n    for i, (img, _, _, all_caps) in enumerate(tqdm(val_loader, desc=f\"üìù Beam Decode ({aspect})\")):\n        img = img.squeeze(0).to(device)\n        gen_caption = caption_image_beam_search(encoder, decoder, img, word_map, beam_size=5, max_len=25)\n        total_words += len(gen_caption)\n        total_unk += gen_caption.count('<unk>')\n\n        hypotheses_bleu.append(gen_caption)\n        caps = all_caps[0].tolist()\n        refs = [[rev_word_map[i] for i in cap if i not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n        references_bleu.append(refs)\n\n        res[i] = [{'caption': ' '.join(gen_caption)}]\n        gts[i] = [{'caption': ' '.join(ref)} for ref in refs]\n\n    bleu1 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n    bleu2 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1)\n    bleu3 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1)\n    bleu4 = corpus_bleu(references_bleu, hypotheses_bleu, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1)\n\n    tokenizer = PTBTokenizer()\n    gts_tok = tokenizer.tokenize(gts)\n    res_tok = tokenizer.tokenize(res)\n\n    rouge_scorer = Rouge()\n    rouge_score, _ = rouge_scorer.compute_score(gts_tok, res_tok)\n\n    meteor_scorer = Meteor()\n    meteor_score, _ = meteor_scorer.compute_score(gts_tok, res_tok)\n\n    cider_scorer = Cider()\n    cider_score, _ = cider_scorer.compute_score(gts_tok, res_tok)\n\n    try:\n        spice_scorer = Spice()\n        spice_score, _ = spice_scorer.compute_score(gts_tok, res_tok)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Skipping SPICE for {aspect} due to error: {e}\")\n        spice_score = -1\n\n    results.append({\n        \"Aspect\": aspect,\n        \"BLEU-1\": bleu1,\n        \"BLEU-2\": bleu2,\n        \"BLEU-3\": bleu3,\n        \"BLEU-4\": bleu4,\n        \"ROUGE-L\": rouge_score,\n        \"METEOR\": meteor_score,\n        \"CIDEr\": cider_score,\n        \"SPICE\": spice_score,\n        \"UNK Ratio\": total_unk / total_words if total_words > 0 else 0\n    })\n\n# Akhir\ndf_results = pd.DataFrame(results)\nprint(\"\\nüìà Full Evaluation Results:\")\ndisplay(df_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:14:08.478599Z","iopub.execute_input":"2025-06-15T04:14:08.478944Z","iopub.status.idle":"2025-06-15T04:27:44.802829Z","shell.execute_reply.started":"2025-06-15T04:14:08.478924Z","shell.execute_reply":"2025-06-15T04:27:44.802234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nbleu_cols = [\"BLEU-1\", \"BLEU-2\", \"BLEU-3\", \"BLEU-4\"]\ndf_bleu = df_results.set_index(\"Aspect\")[bleu_cols]\n\ndf_bleu.plot(kind=\"bar\", figsize=(10, 6), title=\"BLEU-n Scores per Aspect\")\nplt.ylabel(\"Score\")\nplt.ylim(0, 0.25)\nplt.xticks(rotation=30)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:28:06.467013Z","iopub.execute_input":"2025-06-15T04:28:06.467611Z","iopub.status.idle":"2025-06-15T04:28:06.712493Z","shell.execute_reply.started":"2025-06-15T04:28:06.467590Z","shell.execute_reply":"2025-06-15T04:28:06.711799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric_cols = [\"ROUGE-L\", \"METEOR\", \"CIDEr\"]\ndf_metrics = df_results.set_index(\"Aspect\")[metric_cols]\n\ndf_metrics.plot(kind=\"bar\", figsize=(10, 6), title=\"ROUGE, METEOR, CIDEr per Aspect\", color=[\"#66c2a5\", \"#fc8d62\", \"#8da0cb\"])\nplt.ylabel(\"Score\")\nplt.ylim(0, 0.2)\nplt.xticks(rotation=30)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:28:15.050205Z","iopub.execute_input":"2025-06-15T04:28:15.050933Z","iopub.status.idle":"2025-06-15T04:28:15.314672Z","shell.execute_reply.started":"2025-06-15T04:28:15.050902Z","shell.execute_reply":"2025-06-15T04:28:15.313884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json, os, torch\nfrom torchsummary import summary\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nckpt_path = \"/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models/general_impression_best.pth\"\nword_map_path = \"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\"\n\n# Load word_map\nwith open(word_map_path, \"r\") as j:\n    word_map = json.load(j)\nvocab_size = len(word_map)\n\n# Load checkpoint\nckpt = torch.load(ckpt_path, map_location=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:27:16.190834Z","iopub.execute_input":"2025-07-16T06:27:16.191169Z","iopub.status.idle":"2025-07-16T06:27:16.636369Z","shell.execute_reply.started":"2025-07-16T06:27:16.191146Z","shell.execute_reply":"2025-07-16T06:27:16.635564Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Init models\nencoder = EncoderCNN().to(device)\ndecoder = DecoderRNN(\n    embed_size=300,\n    hidden_size=512,\n    vocab_size=vocab_size,\n    pretrained_embeddings=None,\n    freeze_embeddings=False\n).to(device)\n\n# Load weights from checkpoint\nencoder.load_state_dict(ckpt[\"encoder\"])\ndecoder.load_state_dict(ckpt[\"decoder\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:27:19.376670Z","iopub.execute_input":"2025-07-16T06:27:19.377027Z","iopub.status.idle":"2025-07-16T06:27:20.362515Z","shell.execute_reply.started":"2025-07-16T06:27:19.377002Z","shell.execute_reply":"2025-07-16T06:27:20.361568Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"summary(encoder, input_size=(3, 256, 256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:27:22.259344Z","iopub.execute_input":"2025-07-16T06:27:22.259672Z","iopub.status.idle":"2025-07-16T06:27:22.825626Z","shell.execute_reply.started":"2025-07-16T06:27:22.259641Z","shell.execute_reply":"2025-07-16T06:27:22.824814Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 128, 128]           9,408\n       BatchNorm2d-2         [-1, 64, 128, 128]             128\n              ReLU-3         [-1, 64, 128, 128]               0\n         MaxPool2d-4           [-1, 64, 64, 64]               0\n            Conv2d-5           [-1, 64, 64, 64]           4,096\n       BatchNorm2d-6           [-1, 64, 64, 64]             128\n              ReLU-7           [-1, 64, 64, 64]               0\n            Conv2d-8           [-1, 64, 64, 64]          36,864\n       BatchNorm2d-9           [-1, 64, 64, 64]             128\n             ReLU-10           [-1, 64, 64, 64]               0\n           Conv2d-11          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-12          [-1, 256, 64, 64]             512\n           Conv2d-13          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-14          [-1, 256, 64, 64]             512\n             ReLU-15          [-1, 256, 64, 64]               0\n       Bottleneck-16          [-1, 256, 64, 64]               0\n           Conv2d-17           [-1, 64, 64, 64]          16,384\n      BatchNorm2d-18           [-1, 64, 64, 64]             128\n             ReLU-19           [-1, 64, 64, 64]               0\n           Conv2d-20           [-1, 64, 64, 64]          36,864\n      BatchNorm2d-21           [-1, 64, 64, 64]             128\n             ReLU-22           [-1, 64, 64, 64]               0\n           Conv2d-23          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-24          [-1, 256, 64, 64]             512\n             ReLU-25          [-1, 256, 64, 64]               0\n       Bottleneck-26          [-1, 256, 64, 64]               0\n           Conv2d-27           [-1, 64, 64, 64]          16,384\n      BatchNorm2d-28           [-1, 64, 64, 64]             128\n             ReLU-29           [-1, 64, 64, 64]               0\n           Conv2d-30           [-1, 64, 64, 64]          36,864\n      BatchNorm2d-31           [-1, 64, 64, 64]             128\n             ReLU-32           [-1, 64, 64, 64]               0\n           Conv2d-33          [-1, 256, 64, 64]          16,384\n      BatchNorm2d-34          [-1, 256, 64, 64]             512\n             ReLU-35          [-1, 256, 64, 64]               0\n       Bottleneck-36          [-1, 256, 64, 64]               0\n           Conv2d-37          [-1, 128, 64, 64]          32,768\n      BatchNorm2d-38          [-1, 128, 64, 64]             256\n             ReLU-39          [-1, 128, 64, 64]               0\n           Conv2d-40          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-41          [-1, 128, 32, 32]             256\n             ReLU-42          [-1, 128, 32, 32]               0\n           Conv2d-43          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-44          [-1, 512, 32, 32]           1,024\n           Conv2d-45          [-1, 512, 32, 32]         131,072\n      BatchNorm2d-46          [-1, 512, 32, 32]           1,024\n             ReLU-47          [-1, 512, 32, 32]               0\n       Bottleneck-48          [-1, 512, 32, 32]               0\n           Conv2d-49          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-50          [-1, 128, 32, 32]             256\n             ReLU-51          [-1, 128, 32, 32]               0\n           Conv2d-52          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-53          [-1, 128, 32, 32]             256\n             ReLU-54          [-1, 128, 32, 32]               0\n           Conv2d-55          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n             ReLU-57          [-1, 512, 32, 32]               0\n       Bottleneck-58          [-1, 512, 32, 32]               0\n           Conv2d-59          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-60          [-1, 128, 32, 32]             256\n             ReLU-61          [-1, 128, 32, 32]               0\n           Conv2d-62          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-63          [-1, 128, 32, 32]             256\n             ReLU-64          [-1, 128, 32, 32]               0\n           Conv2d-65          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n             ReLU-67          [-1, 512, 32, 32]               0\n       Bottleneck-68          [-1, 512, 32, 32]               0\n           Conv2d-69          [-1, 128, 32, 32]          65,536\n      BatchNorm2d-70          [-1, 128, 32, 32]             256\n             ReLU-71          [-1, 128, 32, 32]               0\n           Conv2d-72          [-1, 128, 32, 32]         147,456\n      BatchNorm2d-73          [-1, 128, 32, 32]             256\n             ReLU-74          [-1, 128, 32, 32]               0\n           Conv2d-75          [-1, 512, 32, 32]          65,536\n      BatchNorm2d-76          [-1, 512, 32, 32]           1,024\n             ReLU-77          [-1, 512, 32, 32]               0\n       Bottleneck-78          [-1, 512, 32, 32]               0\n           Conv2d-79          [-1, 256, 32, 32]         131,072\n      BatchNorm2d-80          [-1, 256, 32, 32]             512\n             ReLU-81          [-1, 256, 32, 32]               0\n           Conv2d-82          [-1, 256, 16, 16]         589,824\n      BatchNorm2d-83          [-1, 256, 16, 16]             512\n             ReLU-84          [-1, 256, 16, 16]               0\n           Conv2d-85         [-1, 1024, 16, 16]         262,144\n      BatchNorm2d-86         [-1, 1024, 16, 16]           2,048\n           Conv2d-87         [-1, 1024, 16, 16]         524,288\n      BatchNorm2d-88         [-1, 1024, 16, 16]           2,048\n             ReLU-89         [-1, 1024, 16, 16]               0\n       Bottleneck-90         [-1, 1024, 16, 16]               0\n           Conv2d-91          [-1, 256, 16, 16]         262,144\n      BatchNorm2d-92          [-1, 256, 16, 16]             512\n             ReLU-93          [-1, 256, 16, 16]               0\n           Conv2d-94          [-1, 256, 16, 16]         589,824\n      BatchNorm2d-95          [-1, 256, 16, 16]             512\n             ReLU-96          [-1, 256, 16, 16]               0\n           Conv2d-97         [-1, 1024, 16, 16]         262,144\n      BatchNorm2d-98         [-1, 1024, 16, 16]           2,048\n             ReLU-99         [-1, 1024, 16, 16]               0\n      Bottleneck-100         [-1, 1024, 16, 16]               0\n          Conv2d-101          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-102          [-1, 256, 16, 16]             512\n            ReLU-103          [-1, 256, 16, 16]               0\n          Conv2d-104          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-105          [-1, 256, 16, 16]             512\n            ReLU-106          [-1, 256, 16, 16]               0\n          Conv2d-107         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-108         [-1, 1024, 16, 16]           2,048\n            ReLU-109         [-1, 1024, 16, 16]               0\n      Bottleneck-110         [-1, 1024, 16, 16]               0\n          Conv2d-111          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-112          [-1, 256, 16, 16]             512\n            ReLU-113          [-1, 256, 16, 16]               0\n          Conv2d-114          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-115          [-1, 256, 16, 16]             512\n            ReLU-116          [-1, 256, 16, 16]               0\n          Conv2d-117         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-118         [-1, 1024, 16, 16]           2,048\n            ReLU-119         [-1, 1024, 16, 16]               0\n      Bottleneck-120         [-1, 1024, 16, 16]               0\n          Conv2d-121          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-122          [-1, 256, 16, 16]             512\n            ReLU-123          [-1, 256, 16, 16]               0\n          Conv2d-124          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-125          [-1, 256, 16, 16]             512\n            ReLU-126          [-1, 256, 16, 16]               0\n          Conv2d-127         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-128         [-1, 1024, 16, 16]           2,048\n            ReLU-129         [-1, 1024, 16, 16]               0\n      Bottleneck-130         [-1, 1024, 16, 16]               0\n          Conv2d-131          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-132          [-1, 256, 16, 16]             512\n            ReLU-133          [-1, 256, 16, 16]               0\n          Conv2d-134          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-135          [-1, 256, 16, 16]             512\n            ReLU-136          [-1, 256, 16, 16]               0\n          Conv2d-137         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-138         [-1, 1024, 16, 16]           2,048\n            ReLU-139         [-1, 1024, 16, 16]               0\n      Bottleneck-140         [-1, 1024, 16, 16]               0\n          Conv2d-141          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-142          [-1, 256, 16, 16]             512\n            ReLU-143          [-1, 256, 16, 16]               0\n          Conv2d-144          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-145          [-1, 256, 16, 16]             512\n            ReLU-146          [-1, 256, 16, 16]               0\n          Conv2d-147         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-148         [-1, 1024, 16, 16]           2,048\n            ReLU-149         [-1, 1024, 16, 16]               0\n      Bottleneck-150         [-1, 1024, 16, 16]               0\n          Conv2d-151          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-152          [-1, 256, 16, 16]             512\n            ReLU-153          [-1, 256, 16, 16]               0\n          Conv2d-154          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-155          [-1, 256, 16, 16]             512\n            ReLU-156          [-1, 256, 16, 16]               0\n          Conv2d-157         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-158         [-1, 1024, 16, 16]           2,048\n            ReLU-159         [-1, 1024, 16, 16]               0\n      Bottleneck-160         [-1, 1024, 16, 16]               0\n          Conv2d-161          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-162          [-1, 256, 16, 16]             512\n            ReLU-163          [-1, 256, 16, 16]               0\n          Conv2d-164          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-165          [-1, 256, 16, 16]             512\n            ReLU-166          [-1, 256, 16, 16]               0\n          Conv2d-167         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-168         [-1, 1024, 16, 16]           2,048\n            ReLU-169         [-1, 1024, 16, 16]               0\n      Bottleneck-170         [-1, 1024, 16, 16]               0\n          Conv2d-171          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-172          [-1, 256, 16, 16]             512\n            ReLU-173          [-1, 256, 16, 16]               0\n          Conv2d-174          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-175          [-1, 256, 16, 16]             512\n            ReLU-176          [-1, 256, 16, 16]               0\n          Conv2d-177         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-178         [-1, 1024, 16, 16]           2,048\n            ReLU-179         [-1, 1024, 16, 16]               0\n      Bottleneck-180         [-1, 1024, 16, 16]               0\n          Conv2d-181          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-182          [-1, 256, 16, 16]             512\n            ReLU-183          [-1, 256, 16, 16]               0\n          Conv2d-184          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-185          [-1, 256, 16, 16]             512\n            ReLU-186          [-1, 256, 16, 16]               0\n          Conv2d-187         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-188         [-1, 1024, 16, 16]           2,048\n            ReLU-189         [-1, 1024, 16, 16]               0\n      Bottleneck-190         [-1, 1024, 16, 16]               0\n          Conv2d-191          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-192          [-1, 256, 16, 16]             512\n            ReLU-193          [-1, 256, 16, 16]               0\n          Conv2d-194          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-195          [-1, 256, 16, 16]             512\n            ReLU-196          [-1, 256, 16, 16]               0\n          Conv2d-197         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-198         [-1, 1024, 16, 16]           2,048\n            ReLU-199         [-1, 1024, 16, 16]               0\n      Bottleneck-200         [-1, 1024, 16, 16]               0\n          Conv2d-201          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-202          [-1, 256, 16, 16]             512\n            ReLU-203          [-1, 256, 16, 16]               0\n          Conv2d-204          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-205          [-1, 256, 16, 16]             512\n            ReLU-206          [-1, 256, 16, 16]               0\n          Conv2d-207         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-208         [-1, 1024, 16, 16]           2,048\n            ReLU-209         [-1, 1024, 16, 16]               0\n      Bottleneck-210         [-1, 1024, 16, 16]               0\n          Conv2d-211          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-212          [-1, 256, 16, 16]             512\n            ReLU-213          [-1, 256, 16, 16]               0\n          Conv2d-214          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-215          [-1, 256, 16, 16]             512\n            ReLU-216          [-1, 256, 16, 16]               0\n          Conv2d-217         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-218         [-1, 1024, 16, 16]           2,048\n            ReLU-219         [-1, 1024, 16, 16]               0\n      Bottleneck-220         [-1, 1024, 16, 16]               0\n          Conv2d-221          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-222          [-1, 256, 16, 16]             512\n            ReLU-223          [-1, 256, 16, 16]               0\n          Conv2d-224          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-225          [-1, 256, 16, 16]             512\n            ReLU-226          [-1, 256, 16, 16]               0\n          Conv2d-227         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-228         [-1, 1024, 16, 16]           2,048\n            ReLU-229         [-1, 1024, 16, 16]               0\n      Bottleneck-230         [-1, 1024, 16, 16]               0\n          Conv2d-231          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-232          [-1, 256, 16, 16]             512\n            ReLU-233          [-1, 256, 16, 16]               0\n          Conv2d-234          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-235          [-1, 256, 16, 16]             512\n            ReLU-236          [-1, 256, 16, 16]               0\n          Conv2d-237         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-238         [-1, 1024, 16, 16]           2,048\n            ReLU-239         [-1, 1024, 16, 16]               0\n      Bottleneck-240         [-1, 1024, 16, 16]               0\n          Conv2d-241          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-242          [-1, 256, 16, 16]             512\n            ReLU-243          [-1, 256, 16, 16]               0\n          Conv2d-244          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-245          [-1, 256, 16, 16]             512\n            ReLU-246          [-1, 256, 16, 16]               0\n          Conv2d-247         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-248         [-1, 1024, 16, 16]           2,048\n            ReLU-249         [-1, 1024, 16, 16]               0\n      Bottleneck-250         [-1, 1024, 16, 16]               0\n          Conv2d-251          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-252          [-1, 256, 16, 16]             512\n            ReLU-253          [-1, 256, 16, 16]               0\n          Conv2d-254          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-255          [-1, 256, 16, 16]             512\n            ReLU-256          [-1, 256, 16, 16]               0\n          Conv2d-257         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-258         [-1, 1024, 16, 16]           2,048\n            ReLU-259         [-1, 1024, 16, 16]               0\n      Bottleneck-260         [-1, 1024, 16, 16]               0\n          Conv2d-261          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-262          [-1, 256, 16, 16]             512\n            ReLU-263          [-1, 256, 16, 16]               0\n          Conv2d-264          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-265          [-1, 256, 16, 16]             512\n            ReLU-266          [-1, 256, 16, 16]               0\n          Conv2d-267         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-268         [-1, 1024, 16, 16]           2,048\n            ReLU-269         [-1, 1024, 16, 16]               0\n      Bottleneck-270         [-1, 1024, 16, 16]               0\n          Conv2d-271          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-272          [-1, 256, 16, 16]             512\n            ReLU-273          [-1, 256, 16, 16]               0\n          Conv2d-274          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-275          [-1, 256, 16, 16]             512\n            ReLU-276          [-1, 256, 16, 16]               0\n          Conv2d-277         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-278         [-1, 1024, 16, 16]           2,048\n            ReLU-279         [-1, 1024, 16, 16]               0\n      Bottleneck-280         [-1, 1024, 16, 16]               0\n          Conv2d-281          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-282          [-1, 256, 16, 16]             512\n            ReLU-283          [-1, 256, 16, 16]               0\n          Conv2d-284          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-285          [-1, 256, 16, 16]             512\n            ReLU-286          [-1, 256, 16, 16]               0\n          Conv2d-287         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-288         [-1, 1024, 16, 16]           2,048\n            ReLU-289         [-1, 1024, 16, 16]               0\n      Bottleneck-290         [-1, 1024, 16, 16]               0\n          Conv2d-291          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-292          [-1, 256, 16, 16]             512\n            ReLU-293          [-1, 256, 16, 16]               0\n          Conv2d-294          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-295          [-1, 256, 16, 16]             512\n            ReLU-296          [-1, 256, 16, 16]               0\n          Conv2d-297         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-298         [-1, 1024, 16, 16]           2,048\n            ReLU-299         [-1, 1024, 16, 16]               0\n      Bottleneck-300         [-1, 1024, 16, 16]               0\n          Conv2d-301          [-1, 256, 16, 16]         262,144\n     BatchNorm2d-302          [-1, 256, 16, 16]             512\n            ReLU-303          [-1, 256, 16, 16]               0\n          Conv2d-304          [-1, 256, 16, 16]         589,824\n     BatchNorm2d-305          [-1, 256, 16, 16]             512\n            ReLU-306          [-1, 256, 16, 16]               0\n          Conv2d-307         [-1, 1024, 16, 16]         262,144\n     BatchNorm2d-308         [-1, 1024, 16, 16]           2,048\n            ReLU-309         [-1, 1024, 16, 16]               0\n      Bottleneck-310         [-1, 1024, 16, 16]               0\n          Conv2d-311          [-1, 512, 16, 16]         524,288\n     BatchNorm2d-312          [-1, 512, 16, 16]           1,024\n            ReLU-313          [-1, 512, 16, 16]               0\n          Conv2d-314            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-315            [-1, 512, 8, 8]           1,024\n            ReLU-316            [-1, 512, 8, 8]               0\n          Conv2d-317           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-318           [-1, 2048, 8, 8]           4,096\n          Conv2d-319           [-1, 2048, 8, 8]       2,097,152\n     BatchNorm2d-320           [-1, 2048, 8, 8]           4,096\n            ReLU-321           [-1, 2048, 8, 8]               0\n      Bottleneck-322           [-1, 2048, 8, 8]               0\n          Conv2d-323            [-1, 512, 8, 8]       1,048,576\n     BatchNorm2d-324            [-1, 512, 8, 8]           1,024\n            ReLU-325            [-1, 512, 8, 8]               0\n          Conv2d-326            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-327            [-1, 512, 8, 8]           1,024\n            ReLU-328            [-1, 512, 8, 8]               0\n          Conv2d-329           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-330           [-1, 2048, 8, 8]           4,096\n            ReLU-331           [-1, 2048, 8, 8]               0\n      Bottleneck-332           [-1, 2048, 8, 8]               0\n          Conv2d-333            [-1, 512, 8, 8]       1,048,576\n     BatchNorm2d-334            [-1, 512, 8, 8]           1,024\n            ReLU-335            [-1, 512, 8, 8]               0\n          Conv2d-336            [-1, 512, 8, 8]       2,359,296\n     BatchNorm2d-337            [-1, 512, 8, 8]           1,024\n            ReLU-338            [-1, 512, 8, 8]               0\n          Conv2d-339           [-1, 2048, 8, 8]       1,048,576\n     BatchNorm2d-340           [-1, 2048, 8, 8]           4,096\n            ReLU-341           [-1, 2048, 8, 8]               0\n      Bottleneck-342           [-1, 2048, 8, 8]               0\nAdaptiveAvgPool2d-343         [-1, 2048, 14, 14]               0\n================================================================\nTotal params: 42,500,160\nTrainable params: 42,274,816\nNon-trainable params: 225,344\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 564.31\nParams size (MB): 162.13\nEstimated Total Size (MB): 727.19\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Dummy input\nbatch_size = 2\nnum_pixels = 14 * 14\nencoder_dim = 2048\ncaption_len = 20\n\ndummy_encoder_out = torch.randn(batch_size, num_pixels, encoder_dim).to(device)\ndummy_captions = torch.randint(0, vocab_size, (batch_size, caption_len)).to(device)\ndummy_caplens = torch.randint(5, caption_len, (batch_size, 1)).to(device)\n\n# Check forward pass\ndecoder.eval()\nwith torch.no_grad():\n    output = decoder(dummy_encoder_out, dummy_captions, dummy_caplens)\n    print(\"Decoder forward pass successful. Output shape:\", output[0].shape)\n\n# Print architecture\nprint(\"\\nDecoderRNN architecture:\")\nprint(decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T06:27:27.938903Z","iopub.execute_input":"2025-07-16T06:27:27.939265Z","iopub.status.idle":"2025-07-16T06:27:28.107431Z","shell.execute_reply.started":"2025-07-16T06:27:27.939241Z","shell.execute_reply":"2025-07-16T06:27:28.106540Z"}},"outputs":[{"name":"stdout","text":"Decoder forward pass successful. Output shape: torch.Size([2, 16, 8842])\n\nDecoderRNN architecture:\nDecoderRNN(\n  (embedding): Embedding(8842, 300)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (init_h): Linear(in_features=2048, out_features=512, bias=True)\n  (init_c): Linear(in_features=2048, out_features=512, bias=True)\n  (lstm): LSTMCell(2348, 512)\n  (fc): Linear(in_features=512, out_features=8842, bias=True)\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# DAE","metadata":{}},{"cell_type":"code","source":"import json\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.spice.spice import Spice\nimport pandas as pd\n\n# Load ground truth (all.json)\nwith open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n    gt_data = json.load(f)[\"images\"]  # <-- fix di sini\n\n# Load DAE output\nwith open(\"/kaggle/input/dae-model/pytorch/default/1/dae_lstm_outputs.json\", \"r\") as f:\n    dae_outputs = json.load(f)\n\n# --- Mapping ID ke nama gambar\nid_to_img = {i: item[\"filename\"] for i, item in enumerate(gt_data)}  # pakai 'filename'\n\n# --- Buat gts dan res pakai nama file gambar sebagai key\ngts, res = {}, {}\nfor item in dae_outputs:\n    img_name = id_to_img.get(item[\"id\"])\n    if img_name:\n        gt_captions = next(x[\"sentences\"] for x in gt_data if x[\"filename\"] == img_name)\n        gts[img_name] = [{\"caption\": s[\"raw\"]} for s in gt_captions]\n        res[img_name] = [{\"caption\": item[\"dae_caption\"]}]\n\n\n# --- Tokenisasi\ntokenizer = PTBTokenizer()\ngts_tok = tokenizer.tokenize(gts)\nres_tok = tokenizer.tokenize(res)\n\n# Sinkronkan key\nvalid_keys = list(set(gts_tok.keys()) & set(res_tok.keys()))\ngts_tok = {k: gts_tok[k] for k in valid_keys}\nres_tok = {k: res_tok[k] for k in valid_keys}\n\n# --- Evaluasi\nbleu = Bleu(4)\nrouge = Rouge()\nmeteor = Meteor()\ncider = Cider()\ntry:\n    spice = Spice()\n    spice_score, _ = spice.compute_score(gts_tok, res_tok)\nexcept:\n    spice_score = None\n\nbleu_scores, _ = bleu.compute_score(gts_tok, res_tok)\nrouge_score, _ = rouge.compute_score(gts_tok, res_tok)\nmeteor_score, _ = meteor.compute_score(gts_tok, res_tok)\ncider_score, _ = cider.compute_score(gts_tok, res_tok)\n\n# --- Hasil\nresults = {\n    \"BLEU-1\": bleu_scores[0],\n    \"BLEU-2\": bleu_scores[1],\n    \"BLEU-3\": bleu_scores[2],\n    \"BLEU-4\": bleu_scores[3],\n    \"ROUGE-L\": rouge_score,\n    \"METEOR\": meteor_score,\n    \"CIDEr\": cider_score,\n    \"SPICE\": spice_score,\n}\n\ndf = pd.DataFrame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:50:31.216450Z","iopub.execute_input":"2025-06-15T04:50:31.217195Z","iopub.status.idle":"2025-06-15T04:51:29.684757Z","shell.execute_reply.started":"2025-06-15T04:50:31.217176Z","shell.execute_reply":"2025-06-15T04:51:29.684078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nüìà Evaluation Results (DAE):\")\nfor k, v in results.items():\n    print(f\"{k}: {v:.4f}\" if isinstance(v, (int, float)) else f\"{k}: {v}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T04:54:39.209436Z","iopub.execute_input":"2025-06-15T04:54:39.210341Z","iopub.status.idle":"2025-06-15T04:54:39.214850Z","shell.execute_reply.started":"2025-06-15T04:54:39.210311Z","shell.execute_reply":"2025-06-15T04:54:39.214232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter metrik yang valid (bukan None)\nfiltered_results = {k: v for k, v in results.items() if v is not None}\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.bar(filtered_results.keys(), filtered_results.values())\nplt.ylabel(\"Score\")\nplt.title(\"Evaluasi Caption - DAE LSTM (tanpa retouch)\")\nplt.ylim(0, max(filtered_results.values()) + 0.05)\n\n# Tambahkan nilai di atas bar\nfor i, (metric, score) in enumerate(filtered_results.items()):\n    plt.text(i, score + 0.005, f\"{score:.3f}\", ha='center')\n\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T05:06:22.982671Z","iopub.execute_input":"2025-06-15T05:06:22.982956Z","iopub.status.idle":"2025-06-15T05:06:23.147989Z","shell.execute_reply.started":"2025-06-15T05:06:22.982937Z","shell.execute_reply":"2025-06-15T05:06:23.147169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport json\n\n# Load data\ndae_data = torch.load('/kaggle/input/dae-model/pytorch/default/1/dae_preprocessed_filtered.pt')\ninput_seqs = dae_data['input_seqs']\nword_map = dae_data['word_map']\n\n# Buat reverse word_map\nrev_word_map = {v: k for k, v in word_map.items()}\n\n# Decode input_seqs\ndae_inputs = []\nfor idx, seq in enumerate(input_seqs):\n    words = []\n    for token_id in seq.tolist():\n        word = rev_word_map.get(token_id, '<unk>')\n        if word == '<end>':\n            break\n        if word not in ['<start>', '<pad>']:\n            words.append(word)\n    caption = ' '.join(words)\n    dae_inputs.append({\n        \"id\": idx,\n        \"caption_input\": caption\n    })\n\n# Simpan ke file JSON\nwith open(\"dae_inputs.json\", \"w\") as f:\n    json.dump(dae_inputs, f, indent=2)\n\nprint(f\"Berhasil menyimpan {len(dae_inputs)} caption input ke dae_inputs.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T18:18:17.898913Z","iopub.execute_input":"2025-07-18T18:18:17.899700Z","iopub.status.idle":"2025-07-18T18:18:18.399778Z","shell.execute_reply.started":"2025-07-18T18:18:17.899674Z","shell.execute_reply":"2025-07-18T18:18:18.399030Z"}},"outputs":[{"name":"stdout","text":"Berhasil menyimpan 6480 caption input ke dae_inputs.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import requests\nimport base64\nimport traceback\n\n# Fungsi encode gambar ke base64\ndef encode_image_base64(img_path):\n    try:\n        with open(img_path, \"rb\") as img_f:\n            return base64.b64encode(img_f.read()).decode('utf-8')\n    except Exception as e:\n        print(f\"[WARNING] Gagal encode gambar {img_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T18:21:12.324865Z","iopub.execute_input":"2025-07-18T18:21:12.325468Z","iopub.status.idle":"2025-07-18T18:21:12.435577Z","shell.execute_reply.started":"2025-07-18T18:21:12.325444Z","shell.execute_reply":"2025-07-18T18:21:12.434867Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\nOPENAI_API_KEY = \"API KEY\"\n\ndef retouch_gpt_api(caption_dae, input_dae, img_path=None, openai_api_key=OPENAI_API_KEY):\n    \"\"\"\n    Mengirim caption DAE, input DAE, dan image (opsional) ke GPT-4o Mini via API.\n    Return hasil retouch atau None jika rate limit.\n    \"\"\"\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {openai_api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    # Prompt\n    prompt = (\n        f\"Here are combined aspect captions from the CNN-LSTM model (may still contain '<unk>'):\\n\"\n        f\"\\\"{input_dae}\\\"\\n\\n\"\n        f\"And this is the DAE-generated caption (may still contain <unk>):\\n\"\n        f\"\\\"{caption_dae}\\\"\\n\\n\"\n        f\"Your task:\\n\"\n        f\"1. Rewrite the DAE caption to be more natural and fluent, but do NOT change its structure or core words.\\n\"\n        f\"2. IMPORTANT: Ensure the final caption **explicitly covers all the key information/aspects** mentioned in the combined CNN-LSTM caption above (general impression, subject, use of camera, color/light, composition, dof/focus)‚Äîeven if briefly, and not necessarily in order.\\n\"\n        f\"3. Do not add new information that is not present in the input/DAE captions. Do not guess objects or details from the image.\\n\"\n        f\"4. Fix <unk> if possible, and merge all aspects into one cohesive sentence.\\n\"\n        f\"Return only the final caption as one simple sentence containing all aspects. No extra words, no extra creativity.\\n\"\n    )\n\n\n    # Build messages\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert assistant for rewriting image captions.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Jika gambar bisa di-base64, tambahkan sebagai attachment (OpenAI API VISION style)\n    img_base64 = encode_image_base64(img_path) if img_path is not None else None\n    if img_base64:\n        # Use OpenAI \"vision\" input format\n        messages.append({\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n            ]\n        })\n    # Build payload\n    payload = {\n        \"model\": \"gpt-4o-mini\",  # atau 'gpt-4o-mini' jika sudah available, biasanya 'gpt-4o'\n        \"messages\": messages,\n        \"max_tokens\": 96,\n        \"temperature\": 0.7,\n    }\n    # Kirim ke API\n    try:\n        response = requests.post(url, headers=headers, json=payload, timeout=40)\n        if response.status_code == 429:\n            print(\"[Rate Limit] OpenAI API rate limit, skip sample.\")\n            return None\n        response.raise_for_status()\n        result = response.json()\n        if \"choices\" in result:\n            # Jika ada image vision, ambil response terakhir\n            reply = result['choices'][-1]['message']['content'].strip()\n            return reply\n        else:\n            print(\"[ERROR] Response tidak mengandung 'choices'\")\n            print(result)\n            return None\n    except Exception as e:\n        print(f\"[ERROR] API call gagal: {e}\\n{traceback.format_exc()}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T18:21:14.376005Z","iopub.execute_input":"2025-07-18T18:21:14.376604Z","iopub.status.idle":"2025-07-18T18:21:14.385001Z","shell.execute_reply.started":"2025-07-18T18:21:14.376581Z","shell.execute_reply":"2025-07-18T18:21:14.384208Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\n\n# Path input dan output\ndae_output_path = \"/kaggle/input/dae-model/pytorch/default/1/dae_lstm_outputs.json\"\ndae_input_path = \"/kaggle/working/dae_inputs.json\"  # berisi dae_input per ID\nsave_path = \"/kaggle/working/retouch_gpt.json\"\n\n# Load caption DAE\nwith open(dae_output_path, \"r\") as f:\n    dae_outputs = json.load(f)\n\n# Load caption_input (gabungan 6 aspek)\nwith open(dae_input_path, \"r\") as f:\n    dae_inputs = json.load(f)  # format: list of {\"id\": ..., \"caption_input\": ...}\n\n# Buat mapping ID ke input caption\nid_to_input = {x[\"id\"]: x[\"caption_input\"] for x in dae_inputs}\n\n# Bangun hasil retouch\nresults = []\nfor item in tqdm(dae_outputs):\n    id_ = item[\"id\"]\n    caption_dae = item[\"dae_caption\"]\n    caption_input = id_to_input.get(id_)\n\n    if not caption_input:\n        continue  # skip jika tidak ada input\n\n    # Panggil API untuk retouch\n    caption_retouch = retouch_gpt_api(caption_dae, caption_input)\n\n    # Skip jika gagal\n    if caption_retouch is None:\n        continue\n\n    results.append({\n        \"id\": id_,\n        \"caption_dae\": caption_dae,\n        \"caption_input\": caption_input,\n        \"caption_retouch\": caption_retouch\n    })\n\n    # Simpan sementara (checkpoint)\n    with open(save_path, \"w\") as f:\n        json.dump(results, f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T18:21:19.162396Z","iopub.execute_input":"2025-07-18T18:21:19.162664Z","iopub.status.idle":"2025-07-18T20:46:37.921524Z","shell.execute_reply.started":"2025-07-18T18:21:19.162643Z","shell.execute_reply":"2025-07-18T20:46:37.920911Z"}},"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6480/6480 [2:25:18<00:00,  1.35s/it]  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import json\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\n\n# Load ground truth (all.json)\nwith open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n    gt_data = json.load(f)[\"images\"]\n\n# Load retouch caption\nwith open(\"/kaggle/working/retouch_gpt.json\", \"r\") as f:\n    retouch_data = json.load(f)\n\n# Buat mapping ID ke nama file gambar\nid_to_filename = {i: item[\"filename\"] for i, item in enumerate(gt_data)}\n\n# Siapkan gts (ground truth) dan res (retouch caption)\ngts, res = {}, {}\nfor item in retouch_data:\n    id_ = item[\"id\"]\n    img_name = id_to_filename.get(id_)\n    if img_name is None:\n        continue\n    # Ground truth caption dari all.json\n    gt_captions = next(x[\"sentences\"] for x in gt_data if x[\"filename\"] == img_name)\n    gts[img_name] = [{\"caption\": s[\"raw\"]} for s in gt_captions]\n    # Caption hasil retouch\n    res[img_name] = [{\"caption\": item[\"caption_retouch\"]}]\n\n# Tokenisasi\ntokenizer = PTBTokenizer()\ngts_tok = tokenizer.tokenize(gts)\nres_tok = tokenizer.tokenize(res)\n\n# Sinkronkan key (harus sama)\nvalid_keys = list(set(gts_tok.keys()) & set(res_tok.keys()))\ngts_tok = {k: gts_tok[k] for k in valid_keys}\nres_tok = {k: res_tok[k] for k in valid_keys}\n\n# Evaluasi\nbleu = Bleu(4)\nrouge = Rouge()\nmeteor = Meteor()\ncider = Cider()\n\nbleu_scores, _ = bleu.compute_score(gts_tok, res_tok)\nrouge_score, _ = rouge.compute_score(gts_tok, res_tok)\nmeteor_score, _ = meteor.compute_score(gts_tok, res_tok)\ncider_score, _ = cider.compute_score(gts_tok, res_tok)\n\n# Print hasil evaluasi\nprint(\"=== Evaluation of Caption Retouch ===\")\nprint(f\"BLEU-1   : {bleu_scores[0]:.4f}\")\nprint(f\"BLEU-2   : {bleu_scores[1]:.4f}\")\nprint(f\"BLEU-3   : {bleu_scores[2]:.4f}\")\nprint(f\"BLEU-4   : {bleu_scores[3]:.4f}\")\nprint(f\"ROUGE-L  : {rouge_score:.4f}\")\nprint(f\"METEOR   : {meteor_score:.4f}\")\nprint(f\"CIDEr    : {cider_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T20:55:48.477266Z","iopub.execute_input":"2025-07-18T20:55:48.477939Z","iopub.status.idle":"2025-07-18T20:57:13.389088Z","shell.execute_reply.started":"2025-07-18T20:55:48.477915Z","shell.execute_reply":"2025-07-18T20:57:13.388324Z"}},"outputs":[{"name":"stderr","text":"PTBTokenizer tokenized 1080434 tokens at 1419965.27 tokens per second.\nPTBTokenizer tokenized 280362 tokens at 670416.67 tokens per second.\n","output_type":"stream"},{"name":"stdout","text":"{'testlen': 246090, 'reflen': 197117, 'guess': [246090, 239610, 233130, 226650], 'correct': [106386, 22253, 3155, 377]}\nratio: 1.2484463541957251\n=== Evaluation of Caption Retouch ===\nBLEU-1   : 0.4323\nBLEU-2   : 0.2004\nBLEU-3   : 0.0816\nBLEU-4   : 0.0308\nROUGE-L  : 0.2431\nMETEOR   : 0.1314\nCIDEr    : 0.0074\n","output_type":"stream"}],"execution_count":10}]}