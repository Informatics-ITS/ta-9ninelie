{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8dff0b9",
   "metadata": {
    "papermill": {
     "duration": 0.004549,
     "end_time": "2025-06-05T13:42:13.121151",
     "exception": false,
     "start_time": "2025-06-05T13:42:13.116602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f260cca",
   "metadata": {
    "papermill": {
     "duration": 0.003758,
     "end_time": "2025-06-05T13:42:13.128582",
     "exception": false,
     "start_time": "2025-06-05T13:42:13.124824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fungsi Training, Evaluasi, dan Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878dd652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:13.137075Z",
     "iopub.status.busy": "2025-06-05T13:42:13.136654Z",
     "iopub.status.idle": "2025-06-05T13:42:22.455482Z",
     "shell.execute_reply": "2025-06-05T13:42:22.454676Z"
    },
    "papermill": {
     "duration": 9.324874,
     "end_time": "2025-06-05T13:42:22.456923",
     "exception": false,
     "start_time": "2025-06-05T13:42:13.132049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Encoder ---\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "        self.encoder_dim = 2048\n",
    "\n",
    "        resnet = models.resnet101(weights='DEFAULT')\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.resnet(images)\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.permute(0, 2, 3, 1)  # (B, 14, 14, 2048)\n",
    "        out = out.view(out.size(0), -1, out.size(-1))  # (B, num_pixels, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "# --- Decoder ---\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim=2048, dropout=0.5,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        else:\n",
    "            self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.init_h = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.init_c = nn.Linear(encoder_dim, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(embed_size + encoder_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, captions, caplens):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)\n",
    "        caplens, sort_ind = caplens.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        captions = captions[sort_ind]\n",
    "        embeddings = self.embedding(captions)\n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        decode_lengths = (caplens - 1).tolist()\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(encoder_out.device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            awe = encoder_out[:batch_size_t].mean(dim=1)\n",
    "            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], awe], dim=1)\n",
    "            h, c = self.lstm(input_lstm, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "\n",
    "        return predictions, captions, decode_lengths, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebcd909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:22.465573Z",
     "iopub.status.busy": "2025-06-05T13:42:22.464901Z",
     "iopub.status.idle": "2025-06-05T13:42:22.618403Z",
     "shell.execute_reply": "2025-06-05T13:42:22.617606Z"
    },
    "papermill": {
     "duration": 0.159013,
     "end_time": "2025-06-05T13:42:22.619748",
     "exception": false,
     "start_time": "2025-06-05T13:42:22.460735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.h = h5py.File(os.path.join(data_folder, f\"{split}_images_{data_name}.hdf5\"), 'r')\n",
    "        self.imgs = self.h['images']\n",
    "        self.cpi = self.h.attrs['captions_per_image']\n",
    "\n",
    "        with open(os.path.join(data_folder, f\"{split}_captions_{data_name}.json\"), 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "        with open(os.path.join(data_folder, f\"{split}_caplength_{data_name}.json\"), 'r') as j:\n",
    "            self.caplens = json.load(j)\n",
    "\n",
    "        self.dataset_size = len(self.captions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return img, caption, caplen\n",
    "        else:\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)]\n",
    "            )\n",
    "            return img, caption, caplen, all_captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf05ee",
   "metadata": {
    "papermill": {
     "duration": 0.00324,
     "end_time": "2025-06-05T13:42:22.626698",
     "exception": false,
     "start_time": "2025-06-05T13:42:22.623458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5fbcb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:22.634873Z",
     "iopub.status.busy": "2025-06-05T13:42:22.634549Z",
     "iopub.status.idle": "2025-06-05T13:42:48.982262Z",
     "shell.execute_reply": "2025-06-05T13:42:48.981394Z"
    },
    "papermill": {
     "duration": 26.353327,
     "end_time": "2025-06-05T13:42:48.983459",
     "exception": false,
     "start_time": "2025-06-05T13:42:22.630132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GloVe loaded. Coverage: 8577\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os, json\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "aspects = [\"color_light\", \"composition\", \"dof_and_focus\"]\n",
    "data_folder = \"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset\"\n",
    "word_map_path = os.path.join(data_folder, \"wordmap_all.json\")\n",
    "glove_path = \"/kaggle/input/glove6b300dtxt/glove.6B.300d.txt\"\n",
    "pretrained_checkpoint = \"/kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth\"\n",
    "output_dir = \"/kaggle/working/fine-tuned-models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "with open(word_map_path, \"r\") as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "vocab_size = len(word_map)\n",
    "embed_size = 300\n",
    "hidden_size = 512\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "patience = 3\n",
    "learning_rate = 1e-4\n",
    "grad_clip = 5.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transform ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- GloVe Embedding Loader ---\n",
    "def load_glove_embeddings(glove_path, word_map, embedding_dim=300):\n",
    "    import numpy as np\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    embedding_matrix = np.random.uniform(-0.1, 0.1, (len(word_map), embedding_dim)).astype(np.float32)\n",
    "    for word, idx in word_map.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "    print(\"✅ GloVe loaded. Coverage:\", sum([1 for w in word_map if w in embeddings_index]))\n",
    "    return torch.tensor(embedding_matrix)\n",
    "\n",
    "embedding_matrix = load_glove_embeddings(glove_path, word_map, embed_size)\n",
    "\n",
    "# --- Load from pretrained checkpoint ---\n",
    "def load_pretrained_model(embed_matrix, ckpt_path):\n",
    "    encoder = EncoderCNN().to(device)\n",
    "    decoder = DecoderRNN(\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        pretrained_embeddings=embed_matrix\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    # Load encoder state dict normally (semua layer seharusnya match)\n",
    "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "\n",
    "    # Load decoder state dict *selectively*\n",
    "    decoder_state = checkpoint[\"decoder\"]\n",
    "    model_state = decoder.state_dict()\n",
    "\n",
    "    # Filter matching keys\n",
    "    filtered_state = {k: v for k, v in decoder_state.items() if k in model_state and v.size() == model_state[k].size()}\n",
    "    model_state.update(filtered_state)\n",
    "    decoder.load_state_dict(model_state)\n",
    "\n",
    "    print(f\"✅ Partially loaded decoder from {ckpt_path} ({len(filtered_state)} layers matched)\")\n",
    "    return encoder, decoder\n",
    "\n",
    "# --- Training Utilities ---\n",
    "def train_epoch(loader, encoder, decoder, criterion, enc_opt, dec_opt, device):\n",
    "    encoder.train(); decoder.train(); total_loss = 0\n",
    "    for imgs, caps, caplens in tqdm(loader, desc=\"🔥 Training\", leave=False):\n",
    "        imgs, caps, caplens = imgs.to(device), caps.to(device), caplens.to(device)\n",
    "        enc_out = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, _ = decoder(enc_out, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        scores_packed = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets_packed = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        loss = criterion(scores_packed, targets_packed)\n",
    "        dec_opt.zero_grad(); enc_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
    "        dec_opt.step(); enc_opt.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(loader, encoder, decoder, criterion, device):\n",
    "    encoder.eval(); decoder.eval(); total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, caps, caplens, _ in tqdm(loader, desc=\"🔍 Evaluating\", leave=False):\n",
    "            imgs, caps, caplens = imgs.to(device), caps.to(device), caplens.to(device)\n",
    "            enc_out = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, _ = decoder(enc_out, caps, caplens)\n",
    "            targets = caps_sorted[:, 1:]\n",
    "            scores_packed = nn.utils.rnn.pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "            targets_packed = nn.utils.rnn.pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "            loss = criterion(scores_packed, targets_packed)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# --- Save checkpoint ---\n",
    "def save_checkpoint(aspect, epoch, encoder, decoder, enc_opt, dec_opt, val_loss, previous_ckpt_path=None):\n",
    "    ckpt_path = os.path.join(output_dir, f\"{aspect}_best.pth\")\n",
    "    \n",
    "    # Hapus checkpoint lama jika ada\n",
    "    if previous_ckpt_path and os.path.exists(previous_ckpt_path):\n",
    "        os.remove(previous_ckpt_path)\n",
    "        print(f\"🧹 Removed previous checkpoint: {previous_ckpt_path}\")\n",
    "\n",
    "    # Simpan checkpoint baru\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"encoder\": encoder.state_dict(),\n",
    "        \"decoder\": decoder.state_dict(),\n",
    "        \"encoder_optimizer\": enc_opt.state_dict(),\n",
    "        \"decoder_optimizer\": dec_opt.state_dict()\n",
    "    }\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(f\"💾 Saved: {ckpt_path}\")\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79745c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:42:48.991978Z",
     "iopub.status.busy": "2025-06-05T13:42:48.991573Z",
     "iopub.status.idle": "2025-06-05T16:37:38.307769Z",
     "shell.execute_reply": "2025-06-05T16:37:38.306739Z"
    },
    "papermill": {
     "duration": 10489.321881,
     "end_time": "2025-06-05T16:37:38.309157",
     "exception": false,
     "start_time": "2025-06-05T13:42:48.987276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Fine-tuning for aspect: COLOR_LIGHT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
      "100%|██████████| 171M/171M [00:00<00:00, 210MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.8168 | 🔍 Val: 5.6112\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.6089 | 🔍 Val: 5.3453\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.2996 | 🔍 Val: 5.1807\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.0537 | 🔍 Val: 5.0743\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.8385 | 🔍 Val: 5.0010\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.6439 | 🔍 Val: 4.9546\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.4705 | 🔍 Val: 4.9285\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.3094 | 🔍 Val: 4.9126\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.1617 | 🔍 Val: 4.9106\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0185 | 🔍 Val: 4.8960\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/color_light_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.8880 | 🔍 Val: 4.9094\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.7641 | 🔍 Val: 4.9449\n",
      "⚠️ No improvement (2/3)\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.6455 | 🔍 Val: 4.9348\n",
      "⚠️ No improvement (3/3)\n",
      "🛑 Early stopping triggered.\n",
      "\n",
      "📌 Fine-tuning for aspect: COMPOSITION\n",
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.8546 | 🔍 Val: 5.6831\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.6914 | 🔍 Val: 5.4317\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.3943 | 🔍 Val: 5.2669\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.1426 | 🔍 Val: 5.1543\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.9381 | 🔍 Val: 5.0746\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.7487 | 🔍 Val: 5.0236\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.5776 | 🔍 Val: 4.9948\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.4153 | 🔍 Val: 4.9767\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.2732 | 🔍 Val: 4.9786\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.1428 | 🔍 Val: 4.9610\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/composition_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0109 | 🔍 Val: 4.9666\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.8974 | 🔍 Val: 4.9970\n",
      "⚠️ No improvement (2/3)\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.7866 | 🔍 Val: 5.0021\n",
      "⚠️ No improvement (3/3)\n",
      "🛑 Early stopping triggered.\n",
      "\n",
      "📌 Fine-tuning for aspect: DOF_AND_FOCUS\n",
      "✅ Partially loaded decoder from /kaggle/input/pretrained-coco-food-cnn-lstm/pytorch/default/1/pretrain_coco_food_epoch4.pth (8 layers matched)\n",
      "\n",
      "🔁 Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 6.9238 | 🔍 Val: 5.7556\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.7347 | 🔍 Val: 5.5056\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.4134 | 🔍 Val: 5.3362\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 5.1458 | 🔍 Val: 5.2221\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.9141 | 🔍 Val: 5.1502\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.7087 | 🔍 Val: 5.1076\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.5236 | 🔍 Val: 5.0821\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.3562 | 🔍 Val: 5.0674\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.1991 | 🔍 Val: 5.0716\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 4.0545 | 🔍 Val: 5.0669\n",
      "🧹 Removed previous checkpoint: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "💾 Saved: /kaggle/working/fine-tuned-models/dof_and_focus_best.pth\n",
      "\n",
      "🔁 Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.9206 | 🔍 Val: 5.0856\n",
      "⚠️ No improvement (1/3)\n",
      "\n",
      "🔁 Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.7902 | 🔍 Val: 5.0992\n",
      "⚠️ No improvement (2/3)\n",
      "\n",
      "🔁 Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 3.6664 | 🔍 Val: 5.1000\n",
      "⚠️ No improvement (3/3)\n",
      "🛑 Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for aspect in aspects:\n",
    "    print(f\"\\n📌 Fine-tuning for aspect: {aspect.upper()}\")\n",
    "    train_loader = DataLoader(\n",
    "        CaptionDataset(data_folder, aspect, split='train', transform=transform),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        CaptionDataset(data_folder, aspect, split='val', transform=transform),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    encoder, decoder = load_pretrained_model(embedding_matrix, pretrained_checkpoint)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    enc_opt = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=learning_rate)\n",
    "    dec_opt = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    best_ckpt_path = None\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n🔁 Epoch {epoch}/{num_epochs}\")\n",
    "        train_loss = train_epoch(train_loader, encoder, decoder, criterion, enc_opt, dec_opt, device)\n",
    "        val_loss = evaluate(val_loader, encoder, decoder, criterion, device)\n",
    "        print(f\"✅ Train: {train_loss:.4f} | 🔍 Val: {val_loss:.4f}\")\n",
    "    \n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_ckpt_path = save_checkpoint(aspect, epoch, encoder, decoder, enc_opt, dec_opt, val_loss, best_ckpt_path)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print(f\"⚠️ No improvement ({no_improve}/{patience})\")\n",
    "            if no_improve >= patience:\n",
    "                print(\"🛑 Early stopping triggered.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fee59",
   "metadata": {
    "papermill": {
     "duration": 0.527478,
     "end_time": "2025-06-05T16:37:39.449372",
     "exception": false,
     "start_time": "2025-06-05T16:37:38.921894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bc987e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:40.572769Z",
     "iopub.status.busy": "2025-06-05T16:37:40.572103Z",
     "iopub.status.idle": "2025-06-05T16:37:40.578658Z",
     "shell.execute_reply": "2025-06-05T16:37:40.577942Z"
    },
    "papermill": {
     "duration": 0.608863,
     "end_time": "2025-06-05T16:37:40.579799",
     "exception": false,
     "start_time": "2025-06-05T16:37:39.970936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# fine_tune_ckpt = \"/kaggle/input/single-aspects/pytorch/default/2/fine-tuned-models\"\n",
    "# aspects = [\"color_light\", \"composition\", \"dof_and_focus\"]\n",
    "\n",
    "# def caption_image_beam_search(encoder, decoder, image, word_map, beam_size=5, max_len=25):\n",
    "#     k = beam_size\n",
    "#     vocab_size = len(word_map)\n",
    "#     rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "#     encoder_out = encoder(image.unsqueeze(0))  # (1, num_pixels, encoder_dim)\n",
    "#     encoder_dim = encoder_out.size(-1)\n",
    "#     encoder_out = encoder_out.expand(k, -1, encoder_dim)\n",
    "\n",
    "#     k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n",
    "#     seqs = k_prev_words\n",
    "#     top_k_scores = torch.zeros(k, 1).to(device)\n",
    "\n",
    "#     complete_seqs = []\n",
    "#     complete_seqs_scores = []\n",
    "\n",
    "#     h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "#     step = 1\n",
    "#     while True:\n",
    "#         embeddings = decoder.embedding(k_prev_words).squeeze(1)\n",
    "#         awe = encoder_out.mean(dim=1)  # mean attention\n",
    "#         input_lstm = torch.cat([embeddings, awe], dim=1)\n",
    "#         h, c = decoder.lstm(input_lstm, (h, c))\n",
    "#         scores = decoder.fc(h)\n",
    "#         scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "#         scores = top_k_scores.expand_as(scores) + scores\n",
    "#         if step == 1:\n",
    "#             top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "#         else:\n",
    "#             top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
    "\n",
    "#         prev_word_inds = top_k_words // vocab_size\n",
    "#         next_word_inds = top_k_words % vocab_size\n",
    "\n",
    "#         seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "\n",
    "#         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "#         complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "#         if len(complete_inds) > 0:\n",
    "#             complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "#             complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "\n",
    "#         k -= len(complete_inds)\n",
    "#         if k == 0 or step > max_len:\n",
    "#             break\n",
    "\n",
    "#         seqs = seqs[incomplete_inds]\n",
    "#         h = h[prev_word_inds[incomplete_inds]]\n",
    "#         c = c[prev_word_inds[incomplete_inds]]\n",
    "#         encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "#         top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "#         k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "#         step += 1\n",
    "\n",
    "#     if len(complete_seqs) == 0:\n",
    "#     # fallback to the current best in seqs\n",
    "#         best_seq = seqs[0].tolist()\n",
    "#     else:\n",
    "#         i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "#         best_seq = complete_seqs[i]\n",
    "\n",
    "#     decoded = [rev_word_map[idx] for idx in best_seq if idx not in {word_map['<start>'], word_map['<pad>'], word_map['<end>']}]\n",
    "#     return decoded\n",
    "\n",
    "# # --- Evaluasi Semua Aspek ---\n",
    "# bleu_scores = {}\n",
    "\n",
    "# # Load word_map & reverse-nya\n",
    "# with open(word_map_path, \"r\") as f:\n",
    "#     word_map = json.load(f)\n",
    "# rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "# from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# results = []\n",
    "# chencherry = SmoothingFunction()\n",
    "\n",
    "# for aspect in aspects:\n",
    "#     print(f\"\\n📊 Evaluating aspect: {aspect.upper()}\")\n",
    "#     val_loader = DataLoader(\n",
    "#         CaptionDataset(data_folder, aspect, split='val', transform=transform),\n",
    "#         batch_size=1, shuffle=False\n",
    "#     )\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#     encoder.eval(); decoder.eval()\n",
    "\n",
    "#     references, hypotheses = [], []\n",
    "#     total_unk = 0\n",
    "#     total_words = 0\n",
    "\n",
    "#     for img, _, _, all_caps in tqdm(val_loader, desc=f\"📝 Beam Decode ({aspect})\"):\n",
    "#         img = img.squeeze(0).to(device)\n",
    "#         gen_caption = caption_image_beam_search(encoder, decoder, img, word_map, beam_size=5, max_len=25)\n",
    "#         hypotheses.append(gen_caption)\n",
    "#         total_words += len(gen_caption)\n",
    "#         total_unk += gen_caption.count('<unk>')\n",
    "\n",
    "#         caps = all_caps[0].tolist()\n",
    "#         refs = [[rev_word_map[i] for i in cap if i not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n",
    "#         references.append(refs)\n",
    "\n",
    "#     results.append({\n",
    "#         \"Aspect\": aspect,\n",
    "#         \"BLEU-1\": corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-2\": corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-3\": corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1),\n",
    "#         \"BLEU-4\": corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1),\n",
    "#         \"UNK Ratio\": total_unk / total_words if total_words > 0 else 0\n",
    "#     })\n",
    "\n",
    "# import pandas as pd\n",
    "# df_results = pd.DataFrame(results)\n",
    "# print(\"\\n📈 BLEU-n & UNK Summary\")\n",
    "# print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb5298",
   "metadata": {
    "papermill": {
     "duration": 0.530782,
     "end_time": "2025-06-05T16:37:41.635953",
     "exception": false,
     "start_time": "2025-06-05T16:37:41.105171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01b69ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:42.768685Z",
     "iopub.status.busy": "2025-06-05T16:37:42.767999Z",
     "iopub.status.idle": "2025-06-05T16:37:42.772256Z",
     "shell.execute_reply": "2025-06-05T16:37:42.771535Z"
    },
    "papermill": {
     "duration": 0.537148,
     "end_time": "2025-06-05T16:37:42.773443",
     "exception": false,
     "start_time": "2025-06-05T16:37:42.236295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from torchvision.utils import make_grid\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def show_random_predictions(aspects, k=10):\n",
    "#     for aspect in aspects:\n",
    "#         print(f\"\\n🎨 Aspect: {aspect.upper()}\")\n",
    "#         dataset = CaptionDataset(data_folder, aspect, split='test', transform=transform)\n",
    "#         indices = random.sample(range(len(dataset)), k)\n",
    "#         subset = [dataset[i] for i in indices]\n",
    "\n",
    "#         encoder = EncoderCNN().to(device)\n",
    "#         decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "#         ckpt = torch.load(os.path.join(fine_tune_ckpt, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#         encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#         decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#         encoder.eval(); decoder.eval()\n",
    "\n",
    "#         for i, (img, _, _, all_caps) in enumerate(subset):\n",
    "#             img_tensor = img.to(device)\n",
    "#             gen_caption = caption_image_beam_search(encoder, decoder, img_tensor, word_map, beam_size=5, max_len=25)\n",
    "#             caps = all_caps.squeeze(0).tolist()  # shape: (5, max_len) -> list of list\n",
    "#             # references = [[rev_word_map[idx] for idx in cap if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}] for cap in caps]\n",
    "#             references = [\n",
    "#                 [rev_word_map[idx] for idx in cap if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "#                 for cap in caps\n",
    "#             ]\n",
    "\n",
    "#             print(f\"\\n📸 Sample {i+1}\")\n",
    "#             print(\"🔹 Predicted :\", \" \".join(gen_caption))\n",
    "#             print(\"🔸 References:\")\n",
    "#             for r in references[:3]:\n",
    "#                 print(\"  -\", \" \".join(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83819e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:43.898563Z",
     "iopub.status.busy": "2025-06-05T16:37:43.897859Z",
     "iopub.status.idle": "2025-06-05T16:37:43.901324Z",
     "shell.execute_reply": "2025-06-05T16:37:43.900744Z"
    },
    "papermill": {
     "duration": 0.609717,
     "end_time": "2025-06-05T16:37:43.902414",
     "exception": false,
     "start_time": "2025-06-05T16:37:43.292697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show_random_predictions([\"color_light\", \"composition\", \"dof_and_focus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd91f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:44.953751Z",
     "iopub.status.busy": "2025-06-05T16:37:44.953475Z",
     "iopub.status.idle": "2025-06-05T16:37:44.957393Z",
     "shell.execute_reply": "2025-06-05T16:37:44.956843Z"
    },
    "papermill": {
     "duration": 0.534463,
     "end_time": "2025-06-05T16:37:44.958525",
     "exception": false,
     "start_time": "2025-06-05T16:37:44.424062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict_image_caption(image_path, aspect, ckpt_dir, word_map_path, embed_matrix, beam_size=5, max_len=25):\n",
    "#     \"\"\"\n",
    "#     Generate a caption for a given image and aesthetic aspect.\n",
    "#     \"\"\"\n",
    "#     with open(word_map_path, \"r\") as f:\n",
    "#         word_map = json.load(f)\n",
    "\n",
    "#     vocab_size = len(word_map)\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(\n",
    "#         embed_size=embed_matrix.shape[1],\n",
    "#         hidden_size=512,\n",
    "#         vocab_size=vocab_size,\n",
    "#         pretrained_embeddings=embed_matrix,\n",
    "#         freeze_embeddings=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(ckpt_dir, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "#     encoder.eval()\n",
    "#     decoder.eval()\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).to(device)\n",
    "\n",
    "#     caption_ids = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "#     caption_text = \" \".join(caption_ids)\n",
    "#     print(f\"📷 {aspect.upper()} - Caption:\")\n",
    "#     print(f\"📝 {caption_text}\")\n",
    "#     return caption_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc47d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:46.117618Z",
     "iopub.status.busy": "2025-06-05T16:37:46.117082Z",
     "iopub.status.idle": "2025-06-05T16:37:46.120685Z",
     "shell.execute_reply": "2025-06-05T16:37:46.119988Z"
    },
    "papermill": {
     "duration": 0.531859,
     "end_time": "2025-06-05T16:37:46.121787",
     "exception": false,
     "start_time": "2025-06-05T16:37:45.589928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_image_caption(\n",
    "#     image_path=\"/kaggle/input/dpchallenge-images-food-gallery/images/1000368.jpg\",  # atau path gambar lokal\n",
    "#     aspect=\"color_light\",\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects/pytorch/default/2/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix,  # variabel yang kamu sudah punya sebelumnya\n",
    "#     beam_size=5,\n",
    "#     max_len=25\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49203325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:47.237761Z",
     "iopub.status.busy": "2025-06-05T16:37:47.237491Z",
     "iopub.status.idle": "2025-06-05T16:37:47.240805Z",
     "shell.execute_reply": "2025-06-05T16:37:47.240294Z"
    },
    "papermill": {
     "duration": 0.528923,
     "end_time": "2025-06-05T16:37:47.241895",
     "exception": false,
     "start_time": "2025-06-05T16:37:46.712972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix):\n",
    "#     with open(word_map_path, 'r') as j:\n",
    "#         word_map = json.load(j)\n",
    "#     vocab_size = len(word_map)\n",
    "\n",
    "#     encoder = EncoderCNN().to(device)\n",
    "#     decoder = DecoderRNN(\n",
    "#         embed_size=embed_size,\n",
    "#         hidden_size=hidden_size,\n",
    "#         vocab_size=vocab_size,\n",
    "#         pretrained_embeddings=embed_matrix,\n",
    "#         freeze_embeddings=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     ckpt = torch.load(os.path.join(ckpt_dir, f\"{aspect}_best.pth\"), map_location=device)\n",
    "#     encoder.load_state_dict(ckpt[\"encoder\"])\n",
    "#     decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "\n",
    "#     return encoder, decoder, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2001c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:48.378142Z",
     "iopub.status.busy": "2025-06-05T16:37:48.377837Z",
     "iopub.status.idle": "2025-06-05T16:37:48.382148Z",
     "shell.execute_reply": "2025-06-05T16:37:48.381613Z"
    },
    "papermill": {
     "duration": 0.621827,
     "end_time": "2025-06-05T16:37:48.383308",
     "exception": false,
     "start_time": "2025-06-05T16:37:47.761481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def caption_image_beam_search(encoder, decoder, image, word_map, beam_size=5, max_len=25):\n",
    "#     k = beam_size\n",
    "#     vocab_size = len(word_map)\n",
    "#     rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "#     encoder_out = encoder(image.unsqueeze(0))\n",
    "#     encoder_dim = encoder_out.size(-1)\n",
    "#     encoder_out = encoder_out.expand(k, -1, encoder_dim)\n",
    "\n",
    "#     k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)\n",
    "#     seqs = k_prev_words\n",
    "#     top_k_scores = torch.zeros(k, 1).to(device)\n",
    "\n",
    "#     complete_seqs = []\n",
    "#     complete_seqs_scores = []\n",
    "\n",
    "#     h, c = decoder.init_hidden_state(encoder_out)\n",
    "#     step = 1\n",
    "#     while True:\n",
    "#         embeddings = decoder.embedding(k_prev_words).squeeze(1)\n",
    "#         awe = encoder_out.mean(dim=1)\n",
    "#         input_lstm = torch.cat([embeddings, awe], dim=1)\n",
    "#         h, c = decoder.lstm(input_lstm, (h, c))\n",
    "#         scores = decoder.fc(h)\n",
    "#         scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "#         scores = top_k_scores.expand_as(scores) + scores\n",
    "#         if step == 1:\n",
    "#             top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)\n",
    "#         else:\n",
    "#             top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)\n",
    "\n",
    "#         prev_word_inds = top_k_words // vocab_size\n",
    "#         next_word_inds = top_k_words % vocab_size\n",
    "#         seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "\n",
    "#         incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "#         complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "#         if len(complete_inds) > 0:\n",
    "#             complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "#             complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "\n",
    "#         k -= len(complete_inds)\n",
    "#         if k == 0 or step > max_len:\n",
    "#             break\n",
    "\n",
    "#         seqs = seqs[incomplete_inds]\n",
    "#         h = h[prev_word_inds[incomplete_inds]]\n",
    "#         c = c[prev_word_inds[incomplete_inds]]\n",
    "#         encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "#         top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "#         k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "#         step += 1\n",
    "\n",
    "#     if len(complete_seqs_scores) == 0:\n",
    "#         return [\"<unk>\"]\n",
    "\n",
    "#     best_seq = complete_seqs[complete_seqs_scores.index(max(complete_seqs_scores))]\n",
    "#     return [rev_word_map[idx] for idx in best_seq if idx not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc10b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:49.437778Z",
     "iopub.status.busy": "2025-06-05T16:37:49.437531Z",
     "iopub.status.idle": "2025-06-05T16:37:49.441358Z",
     "shell.execute_reply": "2025-06-05T16:37:49.440658Z"
    },
    "papermill": {
     "duration": 0.528925,
     "end_time": "2025-06-05T16:37:49.442548",
     "exception": false,
     "start_time": "2025-06-05T16:37:48.913623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def predict_image_caption(image_path, aspect, ckpt_dir, word_map_path, embed_matrix, beam_size=5, max_len=25):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     image_tensor = transform(image).to(device)\n",
    "\n",
    "#     encoder, decoder, word_map = load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix)\n",
    "#     caption = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "\n",
    "#     print(f\"📷 {os.path.basename(image_path)} — [{aspect.upper()}]\")\n",
    "#     print(f\"📝 Caption: {' '.join(caption)}\")\n",
    "\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"{aspect.upper()} — {' '.join(caption)}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5734328d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:50.568562Z",
     "iopub.status.busy": "2025-06-05T16:37:50.567849Z",
     "iopub.status.idle": "2025-06-05T16:37:50.572211Z",
     "shell.execute_reply": "2025-06-05T16:37:50.571464Z"
    },
    "papermill": {
     "duration": 0.530513,
     "end_time": "2025-06-05T16:37:50.573478",
     "exception": false,
     "start_time": "2025-06-05T16:37:50.042965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def infer_visualize_random_images(aspect, dataset, ckpt_dir, word_map_path, embed_matrix,\n",
    "#                                    test_split=\"test\", beam_size=5, max_len=25, n_samples=10):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 256)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     test_images = [img['filename'] for img in dataset if img['split'] == test_split]\n",
    "#     selected_images = random.sample(test_images, n_samples)\n",
    "\n",
    "#     encoder, decoder, word_map = load_trained_model(aspect, ckpt_dir, word_map_path, embed_matrix)\n",
    "\n",
    "#     for idx, filename in enumerate(selected_images):\n",
    "#         image_path = os.path.join(\"/kaggle/input/dpchallenge-images-food-gallery/images\", filename)\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         image_tensor = transform(image).to(device)\n",
    "\n",
    "#         caption = caption_image_beam_search(encoder, decoder, image_tensor, word_map, beam_size, max_len)\n",
    "#         plt.figure(figsize=(6, 4))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.title(f\"[{aspect.upper()}] — {' '.join(caption)}\", fontsize=10)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f17890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:51.693250Z",
     "iopub.status.busy": "2025-06-05T16:37:51.692550Z",
     "iopub.status.idle": "2025-06-05T16:37:51.695890Z",
     "shell.execute_reply": "2025-06-05T16:37:51.695399Z"
    },
    "papermill": {
     "duration": 0.602631,
     "end_time": "2025-06-05T16:37:51.696973",
     "exception": false,
     "start_time": "2025-06-05T16:37:51.094342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"color_light\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects/pytorch/default/2/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2bc3bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:52.762506Z",
     "iopub.status.busy": "2025-06-05T16:37:52.762200Z",
     "iopub.status.idle": "2025-06-05T16:37:52.765812Z",
     "shell.execute_reply": "2025-06-05T16:37:52.765113Z"
    },
    "papermill": {
     "duration": 0.547092,
     "end_time": "2025-06-05T16:37:52.767000",
     "exception": false,
     "start_time": "2025-06-05T16:37:52.219908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"composition\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects/pytorch/default/2/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be6ff43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:37:53.952573Z",
     "iopub.status.busy": "2025-06-05T16:37:53.951899Z",
     "iopub.status.idle": "2025-06-05T16:37:53.955977Z",
     "shell.execute_reply": "2025-06-05T16:37:53.955276Z"
    },
    "papermill": {
     "duration": 0.544955,
     "end_time": "2025-06-05T16:37:53.957391",
     "exception": false,
     "start_time": "2025-06-05T16:37:53.412436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"/kaggle/input/food-iac-fine-tune-dataset/final/all.json\", \"r\") as f:\n",
    "#     all_data = json.load(f)\n",
    "\n",
    "# # Ambil list gambar\n",
    "# image_list = all_data[\"images\"]\n",
    "\n",
    "# infer_visualize_random_images(\n",
    "#     aspect=\"dof_and_focus\",\n",
    "#     dataset=image_list,\n",
    "#     ckpt_dir=\"/kaggle/input/single-aspects/pytorch/default/2/fine-tuned-models\",\n",
    "#     word_map_path=\"/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json\",\n",
    "#     embed_matrix=embedding_matrix\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5504,
     "sourceId": 8240,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7591480,
     "sourceId": 12061127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7593368,
     "sourceId": 12070627,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 352589,
     "modelInstanceId": 331702,
     "sourceId": 405921,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10548.945887,
   "end_time": "2025-06-05T16:37:57.950916",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T13:42:09.005029",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
