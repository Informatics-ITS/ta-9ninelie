{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport os\nimport json\nfrom tqdm import tqdm\n\n# --- Encoder ---\nclass EncoderCNN(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(EncoderCNN, self).__init__()\n        self.enc_image_size = encoded_image_size\n        self.encoder_dim = 2048\n\n        resnet = models.resnet101(weights='DEFAULT')\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune()\n\n    def forward(self, images):\n        out = self.resnet(images)\n        out = self.adaptive_pool(out)\n        out = out.permute(0, 2, 3, 1)  # (B, 14, 14, 2048)\n        out = out.view(out.size(0), -1, out.size(-1))  # (B, num_pixels, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune\n\n# --- Decoder ---\nclass DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, encoder_dim=2048, dropout=0.5,\n                 pretrained_embeddings=None, freeze_embeddings=False):\n        super(DecoderRNN, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        if pretrained_embeddings is not None:\n            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n            self.embedding.weight.requires_grad = not freeze_embeddings\n        else:\n            self.embedding.weight.data.uniform_(-0.1, 0.1)\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.init_h = nn.Linear(encoder_dim, hidden_size)\n        self.init_c = nn.Linear(encoder_dim, hidden_size)\n        self.lstm = nn.LSTMCell(embed_size + encoder_dim, hidden_size)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, captions, caplens):\n        batch_size = encoder_out.size(0)\n        encoder_out = encoder_out.view(batch_size, -1, self.encoder_dim)\n        caplens, sort_ind = caplens.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        captions = captions[sort_ind]\n        embeddings = self.embedding(captions)\n\n        h, c = self.init_hidden_state(encoder_out)\n        decode_lengths = (caplens - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(encoder_out.device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            awe = encoder_out[:batch_size_t].mean(dim=1)\n            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], awe], dim=1)\n            h, c = self.lstm(input_lstm, (h[:batch_size_t], c[:batch_size_t]))\n            preds = self.fc(self.dropout(h))\n            predictions[:batch_size_t, t, :] = preds\n\n        return predictions, captions, decode_lengths, sort_ind","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:49:05.484978Z","iopub.execute_input":"2025-06-12T20:49:05.485235Z","iopub.status.idle":"2025-06-12T20:49:12.882709Z","shell.execute_reply.started":"2025-06-12T20:49:05.485217Z","shell.execute_reply":"2025-06-12T20:49:12.881969Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision import transforms\n\n# ---- SETUP ----\n# --- List aspek dan folder ---\nckpt_dir1 = '/kaggle/input/single-aspects/pytorch/default/3/fine-tuned-models'\nckpt_dir2 = '/kaggle/input/single-aspects-part-2/pytorch/default/2/fine-tuned-models'\naspects = [\n    \"general_impression\", \"subject\", \"use_of_camera\",\n    \"color_light\", \"composition\", \"dof_and_focus\"\n]\nWORD_MAP_PATH = '/kaggle/input/food-iac-fine-tune-dataset/preprocessed_dataset/wordmap_all.json'\nALL_JSON_PATH = '/kaggle/input/food-iac-fine-tune-dataset/final/all.json'\nIMAGE_DIR = '/kaggle/input/dpchallenge-images-food-gallery/images'\n\nwith open(WORD_MAP_PATH, 'r') as f:\n    word_map = json.load(f)\n\nwith open(ALL_JSON_PATH, 'r') as f:\n    data_json = json.load(f)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:56:57.231858Z","iopub.execute_input":"2025-06-12T20:56:57.232492Z","iopub.status.idle":"2025-06-12T20:56:57.532032Z","shell.execute_reply.started":"2025-06-12T20:56:57.232469Z","shell.execute_reply":"2025-06-12T20:56:57.531498Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- Load Model ---\ndef load_model_for_aspect(aspect):\n    if aspect in [\"color_light\", \"composition\", \"dof_and_focus\"]:\n        ckpt_dir = ckpt_dir1\n    else:\n        ckpt_dir = ckpt_dir2\n\n    checkpoint_path = os.path.join(ckpt_dir, f\"{aspect}_best.pth\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    encoder = EncoderCNN().to(device)\n    decoder = DecoderRNN(\n        embed_size=300,\n        hidden_size=512,\n        vocab_size=len(word_map),\n        pretrained_embeddings=None\n    ).to(device)\n\n    encoder.load_state_dict(checkpoint[\"encoder\"])\n    decoder.load_state_dict(checkpoint[\"decoder\"])\n    encoder.eval()\n    decoder.eval()\n    return encoder, decoder\n\n# --- Load Gambar ---\nfrom torchvision import transforms\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:57:00.447618Z","iopub.execute_input":"2025-06-12T20:57:00.447848Z","iopub.status.idle":"2025-06-12T20:57:00.453578Z","shell.execute_reply.started":"2025-06-12T20:57:00.447832Z","shell.execute_reply":"2025-06-12T20:57:00.452933Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def generate_caption(encoder, decoder, image_tensor, word_map, max_len=30, device='cuda'):\n    \"\"\"\n    Generate caption (greedy decoding) for 1 image_tensor (unsqueezed).\n    \"\"\"\n    # Encode image\n    encoder.eval()\n    decoder.eval()\n    image_tensor = image_tensor.unsqueeze(0).to(device)  # [1, 3, H, W]\n    with torch.no_grad():\n        encoder_out = encoder(image_tensor)   # (1, num_pixels, encoder_dim)\n    num_pixels = encoder_out.size(1)\n    encoder_dim = encoder_out.size(-1)\n    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n\n    # Initialize LSTM state\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    # Store sampled word indices\n    word_idxs = []\n    word = torch.tensor([word_map['<start>']]).to(device)\n    embeddings = decoder.embedding(word)  # (1, embed_dim)\n\n    for t in range(max_len):\n        awe = encoder_out.mean(dim=1)  # global average pooling\n        lstm_input = torch.cat([embeddings, awe], dim=1)\n        h, c = decoder.lstm(lstm_input, (h, c))\n        preds = decoder.fc(h)           # (1, vocab_size)\n        predicted = preds.argmax(1)     # (1,)\n        word_idxs.append(predicted.item())\n        if predicted.item() == word_map['<end>']:\n            break\n        embeddings = decoder.embedding(predicted)\n\n    # Decode to words\n    idx2word = {v: k for k, v in word_map.items()}\n    words = []\n    for idx in word_idxs:\n        word = idx2word.get(idx, '<unk>')\n        if word == '<end>':\n            break\n        if word not in ['<start>', '<pad>']:\n            words.append(word)\n    return ' '.join(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:57:03.205600Z","iopub.execute_input":"2025-06-12T20:57:03.206106Z","iopub.status.idle":"2025-06-12T20:57:03.213024Z","shell.execute_reply.started":"2025-06-12T20:57:03.206085Z","shell.execute_reply":"2025-06-12T20:57:03.212211Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- Load semua model aspek ---\naspect_models = {}\nfor aspect in aspects:\n    encoder, decoder = load_model_for_aspect(aspect)\n    aspect_models[aspect] = (encoder, decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:57:06.433208Z","iopub.execute_input":"2025-06-12T20:57:06.433469Z","iopub.status.idle":"2025-06-12T20:57:44.136408Z","shell.execute_reply.started":"2025-06-12T20:57:06.433453Z","shell.execute_reply":"2025-06-12T20:57:44.135315Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# --------- MAIN LOOP ----------\nall_samples = []\nfor img_info in tqdm(data_json['images']):\n    # Bisa filter split di sini kalau hanya test/val\n    # if img_info['split'] != 'test':\n    #     continue\n    filename = img_info['filename']\n    img_path = os.path.join(IMAGE_DIR, filename)\n    if not os.path.exists(img_path):\n        print(f'Not found: {img_path}')\n        continue\n\n    # Load and preprocess image\n    image = Image.open(img_path).convert('RGB')\n    image_tensor = transform(image)\n\n    # Inference semua aspek\n    aspect_captions = {}\n    for aspect, (encoder, decoder) in aspect_models.items():\n        aspect_captions[aspect] = generate_caption(encoder, decoder, image_tensor.squeeze(0), word_map, device=device)\n\n    # Reference ground truth (semua sentences digabung 1 string, atau pilih satu)\n    reference = [sent['raw'] for sent in img_info['sentences']]\n    all_samples.append({\n        'image_id': filename,\n        'captions': aspect_captions,\n        'reference': reference\n    })\n\n# ---- SIMPAN ----\nwith open('dae_dataset.json', 'w', encoding='utf8') as f:\n    json.dump(all_samples, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T20:58:02.838702Z","iopub.execute_input":"2025-06-12T20:58:02.839394Z","iopub.status.idle":"2025-06-12T21:13:30.331604Z","shell.execute_reply.started":"2025-06-12T20:58:02.839370Z","shell.execute_reply":"2025-06-12T21:13:30.331009Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 7221/7221 [15:27<00:00,  7.79it/s]\n","output_type":"stream"}],"execution_count":11}]}